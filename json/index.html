

[
{
"title": "Exercise 5",
"url": "https://mt2-erlangen.github.io/exercise-5/",
"body": "Submission\nSubmission deadline: 07.06.20 16:15h\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nImage Transformations\nIn the previous exercises, we built a Signal and Image class for performing basic operations on the \ninput data. We also implemented various filters to process the data and remove noise. \nIn this exercise we will build on top of the image class and implement methods for performing image transformations.\nIn many medical applications there is a need to align two images so that we \ncan combine the information between the images. This can be due to the images coming from \ndifferent modalities like (CT and MRI) or in scenarios were you have an patient data at from \ndifferent time (before and after an surgery)  and you want to compare between these two images. \nIn all these scenarios we use image registration bring the different images together.\nIn the below image, two x-ray views (1) and (2) are fused together to obtain the combined view(3)\nwhich produces more information for diagnosis. This is achieved using image registration between view(1) and view\n \nImage Source: Hawkes, David J., et al. &quot;Registration and display of the combined bone scan and \nradiograph in the diagnosis and management of wrist injuries.&quot; European journal of nuclear medicine \n18.9 (1991): 752-756. \nOne of the crucial components of image registration is image transformations.\nIn this exercise we will implement basic image transformations. Additionally, we need to implement an \ninterpolation method to find out the image intensity values at the transformed coordinates. \n\nOverview of tasks\n\nWe will implement the following tasks for this exercise.\n\nHelper functions (a. Image origin, b. Interpolation)\nImage Transformation (a. Translation, b. Rotation, c. Scaling)\n\nWe introduce the basic theory about image transformations in theoretical background section.\nPlease read the theory before proceeding since we don't re-introduce everything in the task description. \nTask Description\n\nWe provide the main method for the task with an interactive ImageJ plug-in in the files\nsrc/main/java/exercises/Exercise05.java\nand src/main/java/mt/ImageTransformer.java\n\n0. Getting started\n1 Point\n\n\nFor Exercise 5 we provide a GUI that displays the image with different image transformation options.\n\n\n\nOnce you have all the transformations implemented you should be able to adjust the sliders and perform the desired transformations in an interactive manner.\n\n\nThe transformations requires an origin point about which we perform all the transformation.\n\n\nExtend the Image class with these three methods\n\n\n\n    // store the origin points x,y as \n    // a class variable\n    public void setOrigin(float x, float y)\n\n    // the origin() returns the {x,y} as float \n    // array from the stored origin class variable. \n    public float[] origin()\n\n    // Sets the origin to the center of the image\n    public void centerOrigin()\n\n\nFurthermore extend the Signal class with the following member which describes the physical distance between two samples\n\n\n    protected float spacing = 1.0f;\n\n\nAdd also a setter and getter method\n\n\n    public void setSpacing(float spacing)\n    public float spacing()\n\n\nWe had a placeholder for the spacing in our show()methods. Replace it with our spacing variable.\n\n\n      public void show() {\n        DisplayUtils.showImage(buffer, name, width(), origin, spacing(), true);\n    }\n      public void show() {\n    DisplayUtils.showArray(buffer, name, /*start index =*/0, spacing);\n    }\n\n\nWe already set the origin point for you in the file src/main/java/exercises/Exercise05.java\nTo ensure that everything is running, run the main function of Exercise05.\n\n1. Image interpolation\n4 Points\n\n\nSince the image transformations heavily relies on the interpolation, we first implement the interpolation method by extending the Image class  with the following method:\n\n\npublic float interpolatedAt(float x, float y)  \n\n\nThe method takes in a physical $(x,y)$ coordinate and returns the image intensity at that position.\nWe use bilinear interpolation to find the value at $(x,y)$ (described in the theory).\n\n\nWe can rewrite the interpolation equation using the linear interpolation formula when we want to interpolate between two points $x_1,x_2$ with function value $f(x_1),f(x_2)$ to find out the function value $f(x)$ at $x$.\n\n\n$$ \\frac{f(x) - f(x_1)}{x-x_1} = \\frac{f(x_2) - f(x_1)}{x_2 - x_1} $$\n\n\n\n\nSince we already know the difference $x_2 - x_1$ is either 1.0 if we have a pixel spacing of 1.0 or pixel spacing, we can simplify the above equation as follows:\n\n$$f(x) = f(x_1) + (x-x_1) (f(x_2) - f(x_1))$$\n\n\n\nYou can use the function below to compute linear interpolation between two points $x_1,x_2$ at $x$\n\n\n // Definition of arguments\n // diff_x_x1 = x - x_1 compute the difference between point x and x_1\n // fx_1 = f(x_1), pixel value at point x_1\n // fx_2 = f(x_2), pixel value at point x_2 \n\n float linearInterpolation(float fx_1, float fx_2, float diff_x_x1) {\n     return fx_1 + diff_x_x1 * (fx_1 - fx_2);\n }\n \n\n\nWe now have an way to interpolate between two points in 1D. We need to extend this to 2D case such that we can use \nit for interpolating values in our image. An illustration of how this can be done is \nalready given in the theory section.\n\n\nImplementation detail We describe here possible way to implement the interpolation scheme.\n\n\nFind the 4 nearest pixel indices, for the given physical coordinate $(x,y)$. To do, this you have to transform\nthe physical coordinate to the index space of the image.\n\n\nHint: In physical space all the values of $x$ and $y$\nare computed from origin. So we just need to subtract the origin from the coordinates for this correction.\n\nx -= origin[0]\ny -= origin[1]\n\n\nPixel spacing also alters the physical coordinates and needs to be corrected for. \nThis can be done using just by dividing each coordinate by the pixel spacing.\n\nx /= spacing;\ny /= spacing\n\n\nHint: Since each pixel is a unit square you can round up and down each coordinate ($x$ and $y$) separately \nto get the 4 nearest pixels coordinates. The Math class from Java provides for that purpose the functions Math.floor and Math.ceil.\n\n\nInterpolate along an axis (here we choose the x-axis) initially using the linear interpolation \nfunction to obtain intermediate points.\n\n\nNow interpolate along the intermediate points (i.e you are interpolating along y-axis)\n\n\nNote: Take care of image origin and pixel spacing for the input coordinates before you perform any of the steps.\nAlso, always use atIndex and setIndex for accessing the image values. \nThis ensures that we handle the values at boundary correctly.\n\n\n\n\nExample:\nHere we look at a single point to understand how to implement our algorithm\n\n\nIf we have an input $(x,y) = (0.4,0.4)$, then the 4 nearest pixel coordinates are $(0,0)$,$(1,0),(1,1),(0,1)$\n\n\nInterpolating the values between the points $a = (0,0)$, $b = (1,0)$, find the intermediate \nvalue at point $I_1 = (0.4,0)$.\n\n\nSimilarly interpolate between $c = (0,1)$ and $d = (1,1)$ to find the intermediate value at point $I_2 = (0.4,1)$.\n\n\nNow we can just use the values at the intermediate points $I_1 = (0.4,0)$ and $I_2 = (0.4,1)$ and \nperform a linear interpolation in the y direction to obtain the final result at $(0.4,0.4)$.\n\n\n\n\n2. Image Transformation\n5 Points\nNow we can start with the implementation of ImageTransformer class.\n\nThe class consists of the following member functions for translation, which will be altered in our apply method.\n\n\n// Transformation parameters\npublic float shiftX; // tx\npublic float shiftY; // ty\npublic float rotation; // theta\npublic float scale; // s\n\n\n\nAlso use the interface ImageFilter abstract class which you have implemented in the previous exercises. \nThis can be done using implements keyword.\n\n\nAdd the method apply(Image input,Image output) which takes in two variables input and \noutput of Image class type. The input variable provides the input image to our transformer class. \nThe output variable is where the transformed image is stored.\n\n\nConsider each pixel in the image with index $(i,j)$. When we access an image pixel we get \nthe pixel intensity stored at the location $(i,j)$.\n\n\nHere $(i,j)$ represents the image coordinates $(x,y)$ and the pixel value at $(i,j)$ represents $f(x,y)$.\n\n\nWe want to transform $(x,y) \\to (x',y')$ and find the pixel value at the new location for a \ngiven set of input transformation parameters $t_x,t_y,\\theta,s$ to transform the input image coordinate $(x,y)$.\n\n\nLet us go over a possible approach to implement the apply method which \nimplements (translation,rotation and scaling). In addition, once we have the transformed coordinates $(x',y')$ we \ninterpolate the value at this coordinate to set the output value of the new image.\n\n\nWe can implement the transformations and interpolation using the equations defined \nin the theory section. \n\n\nHowever, from the implementation perspective it is much easier to ask what will be my output image value \nat the current position $(x',y')$  for the given  transformations parameters.\n\n\nFor this we need to find the input coordinate $(x,y)$ for the given transformation parameters.\nThis mapping from $(x',y') \\to (x,y)$ is known as the inverse transformation.\n\n\nJust to recap our current aim is to iterate over the output image along each \npixel $(i,j)$ (also referred as $(x',y')$) and find the inverse transformation (x,y).\nOnce we find $(x,y)$ we can just interpolate the values in the input image at $(x,y)$ and\nset it to the output image value at (x',y').\n\n\nAn example code to accomplish this looks like below:\n\n\n\n// We need to compute (x,y) from (x&#39;,y&#39;)\n// We use xPrime,yPrime in the code to indicate (x&#39;,y&#39;)\n// Interpolate the values at (x,y) from the input image to get\nfloat pixelValue = input.interpolatedAt(x,y);\n\n// Set your result at the current output pixel (x&#39;,y&#39;)\noutput.setAtIndex(xPrime, yPrime, pixelValue);\n\n\n\n\nThe inverse transformations can be computed using the following equations.\n\n\nTranslation\n\n$x =  x' - t_x$\n$y =  y' - t_y$ \n\n\n\nRotation\n\n$x= x' \\cos\\theta + y' \\sin\\theta$\n$y= - x \\sin\\theta + y' \\cos\\theta$\n\n\n\nScaling\n\n$x=  \\frac{x'}{s}$\n$y=  \\frac{y'}{s}$\n\n\n\nImplementation detail Now you can directly use the above equations to implement translation, rotation and scaling.\nThe entire apply method for the ImageTransformer class can be implemented as follows:\n\n\nIterate over each pixel in the output image (although they are just the same as input initially).\n\n\nAt each pixel the index $(i,j)$ represents our coordinates $(x',y')$ of the output image\n\n\nApply the transformations using the equations described above to find $(x,y)$\n\n\nNow set the output image value at $(i,j)$ (also referred as (x',y')) from the interpolated values at $(x,y)$ \nfrom the input image.\n\n\nUse the setIndex() for setting the values of the output image and atIndex() for getting the values \nfrom input image.\n\n\nIn the above formulation we assume that we have pixel spacing of $spacing = 1.0$ and the \nimage origin at $(x_0, y_0) = (0,0)$.\n\n\nYou can extend this to work for different values of pixel spacing and origin.\n\n\nHint: Think of pixel spacing as a scaling and origin as a translation transformation. \n(apply both spacing and origin transformation to the input coordinates $(x,y)$ as  $(x * px , y * py) + (x_0,y_0))$ \n\n\n\n\nSubmitting\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nYou only need to submit the code. No need to submit answers to the questions in the text.\nThen, compress your source code folder src to a zip archive (src.zip) and submit it via StudOn!\n"
}

{
"title": "Exercise 4",
"url": "https://mt2-erlangen.github.io/exercise-4/",
"body": "Submission deadline: 30.05.21 23:55h\nPlease ensure that all files you created also contain your name and your IDM ID\nand also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\n2D Convoultion and Image Filters\nIn this exercise, we are using our image class from exercise 3 to build the 2D convolution, which you already implement for the 1D case in exercise 2. \nmt.Image Filter\n4  Points:\nLike in Exercise 2, we want to be able to convolve our image signal.\nInfact, we will learn a lot of new ways to process images.\nOften, we need to create an output image of same size.\nLet's create an interface (src/main/java/mt/ImageFilter.java) for that, so we only need to implement this once.\n\npackage mt;\n\npublic interface ImageFilter {\n    default mt.Image apply(mt.Image image) {\n        Image output = new Image(image.width(), image.height(), image.name() + &quot; processed with &quot; + this.name());\n        apply(image, output);\n        return output;\n    }\n\n    default void apply(mt.Image input, mt.Image output) {\n        throw new RuntimeException(&quot;Please implement this method!&quot;);\n    }\n\n    String name();\n}\n\nOk. Now the convolution. The class has already a method normailze that we will need later. It uses a method sum(), which we need to implement in our Signal class. The method sums up all values of our signal:\n\npublic float sum() //&lt; sum of all signal values \n    \n\nThe code for the convolution should go to src/main/java/mt/LinearImageFilter.java\n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\npublic class LinearImageFilter extends Image implements ImageFilter {\n\n    public void normalize() {\n\tdouble sum = sum();\n\tfor (int i = 0; i &lt; buffer.length; i++) {\n\t    buffer[i] /= sum;\n\t}\n    }\n}\n\nCreate a constructor for it. Recall how we implemented LinearFilter!\nminIndexX and minIndexY need to be set to $-\\lfloor L_x/2 \\rfloor$ and $-\\lfloor L_y/2 \\rfloor$ when $L_x$ is the\nfilter's width and $L_y$ the filter's height.\n\n    public LinearImageFilter(int width, int height, String name)\n\nConvolution in 2-d works similar to convolution in 1-d.\n$$K_x = \\lfloor L_x/2 \\rfloor$$\n$$K_y = \\lfloor L_y/2 \\rfloor$$\n$$g[x,y] = \\sum_{y'=-K_y}^{+K_y} \\sum_{x'=-K_x}^{+K_x} f[x-x', y-y'] \\cdot h[ x', y' ] $$\n$$g[x,y] = \\sum_{y'=\\text{h.minIndexY}}^{\\text{h.maxIndexY}} \\sum_{x'=\\text{h.minIndexX}}^{\\text{h.maxIndexX}} f[x-x', y-y'] \\cdot h[ x', y' ] $$\nRemember to use atIndex and setAtIndex to get and set the values.\nImplement the convolution in the method apply.\nThe result image was already created by our interface ImageFilter.\n\n    public void apply(Image image, Image result)\n\n\n\nSource: https://github.com/vdumoulin/conv_arithmetic\nNow it's almost time to test. We just need to add another method to our Signal class:\n\n// Needs: import java.util.Random\npublic void addNoise(float mean, float standardDeviation) {\nRandom rand = new Random();\nfor (int i = 0; i &lt; buffer.length; i++) {\n\t   buffer[i] += mean + rand.nextGaussian() * standardDeviation;\n\t}\n }\n\n\nNow you can use the file src/test/java/mt/LinearImageFilterTests.java.\nGauss Filter\n2 Points\nThe code for the Gauss filter should go to src/main/java/mt/GaussFilter2d.java.\nThe Gauss filter is a LinearImageFilter with special coefficients (the filter has the same height and width).\n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\npublic class GaussFilter2d extends LinearImageFilter {\n    \n}\n\nIt has the following constructor\n\n    public GaussFilter2d(int filterSize, float sigma)\n\nIn the constructor, set the coefficients according to the unormalized 2-d normal distribution with standard deviation $\\sigma$ (sigma).\nMath.exp is the exponetial function.  Use setAtIndex: $x$ should run from minIndexX to maxIndexX and $y$ from minIndexY to maxIndexY.\n$$ h[x,y] = \\frac{1}{2 \\pi \\sigma^2}\\mathrm{e}^{-\\frac{x^2+y^2}{2 \\sigma^2}}$$\nCall normalize() at the end of the constructor to ensure that all coefficients sum up to one.\nTest your Gauss filter in Exercise04.java.\nUse arbitray values for sigma and filterSize.\nThe Gauss filter will blur your input image clearly if you chose a large value for sigma.\n\nThere is also a unit test file that you can use: src/test/java/mt/GaussFilter2dTests.java\nMore Filters!\n4 Points\nLets try implement even more filters, to see in how many ways we can use our LinearImageFilter class!\n\n\nThe code for the Average filter should go to src/main/java/mt/Averagefilter2d.java.\nThe Average filter is a LinearImageFilter with special coefficients (the filter has the same height and width).\n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\npublic class AverageFilter2d extends LinearImageFilter {\n    \n}\n\nIt has the following constructor\n\n    public AverageFilter2d(int filterSize)\n\nUse setAtIndex: $x$ should run from minIndexX to maxIndexX and $y$ from minIndexY to maxIndexY. The filter takes the average around its neighbours.\n$$ h[x,y] = \\frac{1}{filterSize*filterSize}$$\nThe code for the Derivative filter should go to src/main/java/mt/Derivativefilter2d.java.\nThe Derivative filter is a LinearImageFilter and has always a fixed size of $3$.\n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\npublic class DerivativeFilter2d extends LinearImageFilter {\n    \n}\n\nIt has the following constructor, which uses a boolean to termine its direction. The standart should be in x direction.\n\n    public DerivativeFilter2d(boolean transpose)\n\nThe filter computes the derivative of each point in either x or y direction.\n$$ \\partial x = [-1,0,1] , \\partial y = [-1,0,1]^T$$\n\n\nThe last filter we implement is a sharpening filter.\nThe code for the Sharpening filter should go to src/main/java/mt/Sharpeningfilter2d.java.\nThe Sharpening filter is a LinearImageFilter with a kernel size of $3x3$.\n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\npublic class SharpeningFilter2d extends LinearImageFilter {\n    \n}\n\nIt has the following constructor\n\n    public SharpeningFilter2d(float focus)\n\nUse setAtIndex: $x$ should run from minIndexX to maxIndexX and $y$ from minIndexY to maxIndexY. The filter takes the negative average of its neighbourhood and enhances its current position by the value of focus.\n$$ h[x,y] = -\\frac{(focus-1)}{8}$$\n$$ h[0,0] = focus $$\nMy filter results look like this:\n\n\n  \n  \n  \n\n\nGauss Filter \nDerivate Filter in x driection \nDerivate Filter in y driection \n\n\n  \n  \n\n\n\nAverage Filter  \nSharpening Filter \n\n\n \nCan you reproduce them?\nSubmitting\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nThen, compress your source code folder src to a zip archive (src.zip) and submit it on studOn.\n"
}

{
"title": "Exercise 3",
"url": "https://mt2-erlangen.github.io/exercise-3/",
"body": "Submission deadline: 23.05.21 23:55h\nPlease ensure that all files you created also contain your name and your IDM ID\nand also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nImages and 2D Waves\nIn this exercise, we finally work with images. It's time to update the file src/main/java/lme/DisplayUtils.java to the newest version.\nThis should provide you the following methods to work with images:\n\n    // Open a file\n    public static mt.Image openImage(String path) \n\n    // Download and open a file from the internet\n    public static mt.Image openImageFromInternet(String url, String filetype) \n\n    // Save an image to a file\n    public static void saveImage(mt.Image image, String path) \n\n    //  Show images\n    public static void showImage(float[] buffer, String title, int width) \n    public static void showImage(float[] buffer, String title, long width, float[] origin, double spacing, boolean replaceWindowWithSameName)\n    \n    // Calculate a Fourier transform for an image\n    public  static void FFT(float[] buffer, String title, long width, float[] origin, double spacing)\n\nmt/Image.java\n4 Points\nThe code for this section should go to src/main/java/mt/Image.java\nOur goal is to share as much code with our mt.Signal class. So mt.Image will be a subclass of mt.Signal.\n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\nimport lme.DisplayUtils;\n\npublic class Image extends Signal {\n\n\n}\n\nmt.Image has five members (apart from the ones inherited by mt.Signal).\n\n    // Dimensions of the image\n    protected int width; \n    protected int height; \n\n    // Same as Signal.minIndex but for X and Y dimension\n    protected int minIndexX;\n    protected int minIndexY;\n\n    // For a later exercise (no need to do anything with it in exercise 3)\n    protected float[] origin = new float[]{ 0, 0 };\n\nAnd two constructors:\n\n    // Create an image with given dimensions\n    public Image(int width, int height, String name)\n\n    // Create an image with given dimensions and also provide the content\n    public Image(int width, int height, String name, float[] pixels)\n\nAs shown in the exercise slides, we will store all the pixels in one array, like we did in Signal.\nThe array should have the size width * height.\nminIndexX,minIndexY should be 0 for normal images.\n\n\nCall the constructors of the super class Signal in the constructors of Image.\nYou can call the constructor of a super class by placing super(...) with the respetive arguments in the first line of the constructor of the subclass.\nThe constructor public Image(int width, int height, String name, float[] pixels) does not need to create its own array (take pixels for buffer).\nBut you can check whether pixels has the correct size.\nLet's also provide some getters!\n\n    // Image dimensions\n    public int width()\n    public int height()\n\n    // Minimum and maximum indices (should work like Signal.minIndex/maxIndex)\n    public int minIndexX()\n    public int minIndexY()\n    public int maxIndexX()\n    public int maxIndexY()\n\natIndex and setAtIndex should work like in Signal except that they now have two coordinate indices.\natIndex should return 0.0f if either the x or y index are outside of the image ranges.\n\n    public float atIndex(int x, int y)\n    public void setAtIndex(int x, int y, float value) \n    \n    //We need a method to set our buffer for the show() function\n    public void setBuffer(float[] buffer)\n\t\n\nRemember how we calculated the indices in the exercise slides. You have to apply that formula in atIndex/setAtIndex.\n\n\nAdd the method show to display the image\n\n    public void show() {\n        DisplayUtils.showImage(buffer, name, width(), origin, /*spacing()*/ 1.0f, /*Replace window with same name*/true);\n    }\n\nOpen the image pacemaker.png in a file  src/main/java/exercise/Exercise03:\n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage exercises;\n\nimport mt.Image;\n\npublic class Exercise03 {\n    public static void main(String[] args) {\n        (new ij.ImageJ()).exitWhenQuitting(true);\n\n        Image image = lme.DisplayUtils.openImageFromInternet(&quot;https://mt2-erlangen.github.io/pacemaker.png&quot;, &quot;.png&quot;);\n        image.show();\n\n    }\n}\n\nThe image is from our open access book.\n\nYou can check the correctness of atIndex with the ImageTestin the file src/test/java/ImageTests.java. \n2D Waves\n4 Points\nWe implemented a 1D Sin and Cos function in Exercise01. Now we want to build the 2D functions in SineWave2d and CosineWave2d. Create the two files src/main/java/mt/SineWave2d and src/main/java/mt/CosineWave2d. They will be a subclass of mt.Image.\nAdditionally we are building a class Vector2d, which sets the frequency and orientation of our waves. Create therefore the file  src/main/java/mt/Vector2d.\nThe class has two members k_xand k_y and a simple constructor.\n\n\npackage mt;\n\npublic class Vector2d {\n    float kx,ky;\n\n    public Vector2d(float kx, float ky){\n        ...\n\n    }\n    \n}    \n\nTo use the vector in the calculation of our Sine and Cosine waves, we need to implement a function dot to form the dot product of two vectors:\n$$ a \\cdot b = \\sum_{i=1}^{n} a_i b_i$$\n\npublic float dot(Vector2d other)\n\n\n\nThe vector and the waves can be visualized like in the lecture:\n\nThe setup for the 2D wave functions will be the following:\n\n\npackage mt;\n\npublic class SineWave2d extends Image {\n    \n}    \n\nThe min and max indices of the wave functions should be set to $-[\\frac{Width}{2}]$, $[\\frac{Width}{2}]$ and $-[\\frac{Height}{2}]$, $[\\frac{Height}{2}]$.\nBoth classes have a constructor in which you need set the pixel intensity in the buffer with the hep of the vector k:\n\n    public CosineWave2d(Vector2d k, int width, int height, String name)\n    public SineWave2d(Vector2d k, int width, int height, String name)\n    \n\nTo get the intensity of each pixel of the 2D Wave Image we can use the following formulas:\n$$IntensityCos\\left[x,y\\right] = \\cos\\left((\\frac{x}{Width}\\cdot k_x+\\frac{y}{Height}\\cdot k_y) \\cdot 2\\pi\\right) $$\n$$IntensitySin\\left[x,y\\right] = \\sin\\left((\\frac{x}{Width}\\cdot k_x+\\frac{y}{Height}\\cdot k_y) \\cdot 2\\pi\\right) $$\nAdding up waves and looking at the Fourier Transform\n2 Points\nTo add up multiple signals we implement a similar methods like in Exercise 1.\n\npublic Image add(Image image)\n\nThe method add should add up two images element wise. Return the addition with a new image. Be careful with the min and max indices of the images. Check if the images differ in size and throw an exception otherwise. \nCan you create an Image with checkerboard like patterns, by adding up multiple waves?\n\nAdditionally we want to look at the Fourier transformation of our waves. As you saw in the lecture, the Fourier transform analysis the frequency spectrum of our signal. \nAdd the function fft to the mt.image class, which calls a function from DisplayUtils.java:\n\n   public void fft(){\n       DisplayUtils.FFT(buffer, name, width(), origin, /*spacing()*/ 1.0f);\n   }\n\nThe function can be called just like the show()function. Be careful with the image size. The FFT of imageJ works best for our signals with image sizes of $2^n$ ($64x64$,$256x256$,....). \nCan you tell to which wave functions these frequency spectrum belong?\n\n\n  \n  \n\n \nHow does the Fourier transform of our first picture pacemaker.png look like?\n\n\nIn the end you can use the Demo below to test some behavior of 2D Sine and Cosine Waves with different and multiple $k_x , k_y$ parameters. \n\n\n\n\n\nSubmitting\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nYou only need to submit the code. No need to submit answers to the questions in the text.\nThen, compress your source code folder src to a zip archive (src.zip) and submit it via StudOn!\n"
}

{
"title": "Exercise 2",
"url": "https://mt2-erlangen.github.io/exercise-2/",
"body": "Will Corona Ever End?\nSubmission deadline: 16.05.21 23:55h\nPlease ensure that all files you created also contain your name and your IDM ID\nand also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nInfinite Signals!\n3 Points\nRight now our Signal class only represent a signal of finite length.\nNow we want to extend our signal class.\nIn our black board exercises, we agreed that we want to continue our signals with zeros where we don't have any values stored.\nIf we access indices of our Signal with values smaller than minIndex() or larger maxIndex() we want to return 0.0f.\nIf a user accesses an index between minIndex() and maxIndex() we want to return the corresponding value stored in our array.\n\nImplement the method atIndex and setAtIndex and minIndex/maxIndex for Signal.\nPlease be aware that minIndex can be smaller than 0 for subclasses of Signal.\nIf setAtIndex is called with an invalid index (smaller than minIndex or greater than maxIndex), it's ok for the program to crash.\nThis should not happen for atIndex.\nSignal should add the following member\n\nprotected int minIndex; //Index of first array element (should be 0 for signals)\n\nImplement the methods explained above for Signal\t\n\npublic int minIndex() // Get lowest index of signal (that is stored in buffer)\npublic int maxIndex()\t// Get highest index of signal (that is stored in buffer)\t\npublic float atIndex(int i)\npublic void setAtIndex(int i, float value)\n\nYou can check the correctness of atIndex/setAtIndex with the test testAtIndex in file src/test/java/SignalTests.java\nLinearFilter\n3 Points\nImplement LinearFilter in file src/main/java/mt/LinearFilter.java as a subclass of Signal.\nLinearFilter should work like Signal except its minIndex should be at  \n- (coefficients.length/2) as in the exercise slides.\n\nLinearFilter should have a constructor that checks that coefficients is an array of odd size or throws an error otherwise (any error is ok).\n\n    public LinearFilter(float[] coefficients, String name)\n\nand a method that executes the discrete convolution on another Signal input and returns an output of same size.\n\n   public Signal apply(Signal input);\n\nYou should be able to directly use the formula from the exercise slides (f is the input signal, h our filter, $L$ the filter length)\n$$K = \\lfloor L/2 \\rfloor$$\n$$g[k] = \\sum_{\\kappa=-K}^{K} f[k-\\kappa] \\cdot h[ \\kappa ]$$\nor with our minIndex/maxIndex methods for each index $k$ of the output signal.\n$$g[k] = \\sum_{\\kappa=h.\\text{minIndex}}^{h.\\text{maxIndex}} f[k-\\kappa] \\cdot h[\\kappa] $$\nBe sure that you use atIndex to access the values of input and the filter.\n\nYou can test your convolution function with the tests provided in src/test/java/LinearFilterTests.java. For the test you will also need to download the file src/main/java/lme/Algorithms.java. \nGood test cases are:\n\n{0,0,1,0,0}: this filter should not change your signal at all\n{0,1,0,0,0}: this filter should move your signal one value to the left\n{0,0,0,1,0}: this filter should move your signal one value to the right\n\nGet the Current RKI Data\n4 Points\nTo see whether the Corona situation will ever improve we want to have a look at newest data from RKI institute.\nOpen the file \n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npublic static void main(String[] args) throws MalformedURLException, IOException {\n    JSONObject response = readJsonFromUrl(&quot;https://api.corona-zahlen.org/germany/history/cases&quot;);\n\n}\n\nresponse will contain a JSONObject from the org.json library\n(it was installed by adding implementation 'org.json:json:20201115' to your build.gradle). The template for Exercise 2 to use the function above can be downloaded from here: Exercise2.java\nIt contains the newest data from the Robert-Koch-Institute (via https://github.com/marlon360/rki-covid-api) with the following JSON schema:\n\n{\n  &quot;data&quot;: [\n    {\n      &quot;cases&quot;: 1,\n      &quot;date&quot;: &quot;2020-01-07T00:00:00.000Z&quot;\n    },\n    {\n      &quot;cases&quot;: 1,\n      &quot;date&quot;: &quot;2020-01-19T00:00:00.000Z&quot;\n    },\n    {\n      &quot;cases&quot;: 1,\n      &quot;date&quot;: &quot;2020-01-23T00:00:00.000Z&quot;\n    },\n    {\n      &quot;cases&quot;: 1,\n      &quot;date&quot;: &quot;2020-01-25T00:00:00.000Z&quot;\n    },\n    {\n      &quot;cases&quot;: 2,\n      &quot;date&quot;: &quot;2020-01-28T00:00:00.000Z&quot;\n    },\n    {\n      &quot;cases&quot;: 2,\n      &quot;date&quot;: &quot;2020-01-29T00:00:00.000Z&quot;\n    },\n    //...\n  ]\n}\n\nYes, on 2020-01-07 the situation was still pretty good. But what is the situation right now?\nCreate a Signal from the JSONObject with the number of cases for each day! Display the signal with\nits show() method.\n\nWell this looks very wavy... Let's filter this signal!\nApply the following filters on your signal:\n$$ h_1 = \\left[ \\frac{1.0}{7.0}, \\frac{1.0}{7.0}, \\frac{1.0}{7.0}, \\frac{1.0}{7.0}, \\frac{1.0}{7.0}, \\frac{1.0}{7.0}, \\frac{1.0}{7.0}\\right] $$\n\n$$ h_2 = \\left[ 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0 \\right]$$\n\nWhat happens when you apply $h_1$ and then $h_2$? What happens when you first apply $h_2$ and then $h_1$? \nWhat do the individual filters calculate?\nSubmitting\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nYou only need to submit the code. No need to submit answers to the questions in the text.\nThen, compress your source code folder src to a zip archive (src.zip) and submit it via StudOn!\n"
}

{
"title": "Exercise 1",
"url": "https://mt2-erlangen.github.io/exercise-1/",
"body": "Signals and Waves\nSubmission deadline: 09.05.21 23:59h\nPlease ensure that all files you created also contain your name and your IDM ID\nand also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nSignals\n3 Points\nAs a first step, we will implement the class Signal \nwhich should hold a signal of finite length.\nCreate the file src/main/java/mt/Signal.java.\n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\nimport lme.DisplayUtils;\nimport ij.gui.Plot;\n\npublic class Signal {\n\n}\n\nSignal should have the following members\n\n    protected float[] buffer; // Array to store signal values\n    protected String name;    // Name of the signal\n\nImplement two constructors for Signal\n\n    public Signal(int length, String name)     // Create signal with a certain length (set values later)\n    public Signal(float[] buffer, String name) // Create a signal from a provided array\n\nImplement the following getter methods for Signal\n\n    public int size()        // Size of the signal\n    public float[] buffer()  // Get the internal array \n    public String name()     // Get the name of the signal\n\nNext, we want to visualize our Signal in the method show. You can use provided function lme.DisplayUtils.showArray.\nThe file DisplayUtils.java can be downloaded:https://github.com/mt2-erlangen/exercises-ss2021/blob/main/src/main/java/lme/DisplayUtils.java\nTo test it, create a Signal with arbitray values in the main method of src/main/java/exercises/Exercise01.java and call its show method.\n\n    public void show() {\n        DisplayUtils.showArray(this.buffer, this.name, /*start index=*/0, /*distance between values=*/1);\n    }\nWaves\n4 Points\nAs a first step, we will implement the classes CosineWave and SineWave\nwhich should hold a signal of finite length. CosineWave and SineWave inherit from Signal.\nCreate two files src/main/java/mt/CosineWave.java/src/main/java/mt/SineWave.java.\nThe two classes should represent the following two signals:\n$$a\\left[i\\right] = \\cos\\left(\\frac{k}{N}\\cdot 2\\pi\\right) $$\n$$b\\left[i\\right] = \\sin\\left(\\frac{k}{N}\\cdot 2\\pi\\right) $$\n$N$ should be the length (number of samples) of the signal and $k$ is parameter. Add $k$ and $N$ to the constructors of the two classes.\nConstruct some objects of the two classes in src/main/java/exercises/Exercise01.java. How does $k$ influence the shape of the signal?\nThe Perfect Wave\n3 Points\nProfessor Maier said that you can create any signal just using sine and cosine waves.\nTry to generate this signal by adding and scaling some sine and cosine waves (only needs to be similar).\n\nAdd to methods to the signal class in order to add and multipy signals.\nThe methods should not modify the original signal but create a new signal with meaningful name.\n\n    // Adds the signal with another one elementwise\n    // Check that both signals have the same size! You can throw an error otherwise.\n    public Signal plus(Signal other)\n\n    // Multiplies the signal with a scalar\n    public Signal times(float scalar)\n\nHow many sine and cosine waves with what coefficients do you need?\nYou can solve this exercise by trial and error. Maybe some part of the lecture is also helpful...\n\nAnd another one?\nPS: I wouldn't trust the factor $\\frac{2}{5\\pi}$ in the video.\nJust omit the $\\pi$. It's already fixed in your slides on studOn.\nSubmitting\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nYou only need to submit the code. No need to submit answers to the questions in the text.\nThen, compress your source code folder src to a zip archive (src.zip) and submit it via StudOn!\n"
}

{
"title": "Exercise 0",
"url": "https://mt2-erlangen.github.io/exercise-0/",
"body": "Getting Started\nNo submission necessary\nThis is just a test to make sure your Java installation is working before the actual exercises start.\nThe actual exercises start April 26th.\nIf you encounter any problem please create a post in the channel &quot;Java and Project Installation&quot; on Microsoft Teams.\nThere will be a video conference for questions regarding the installtion of Java and ImageJ on this channel on April 23th 10h.\nIf everything is working for you, you just need to wait for next week when the actual exercises start.\nImageJ\nThe image processing program we want to use during this semester is called ImageJ.\nIt was developed at the US National Institutes of Health and is used nowadays especially in research \nfor medical and biological images.\nIf you want to, you can download a stand-alone version of the program here.\nThis is not necessary for the exercises.\nGetting started\nImageJ can also be used as a Java library.\nWe already created a Java project that uses ImageJ.\nYou can download it from https://github.com/mt2-erlangen/exercises-ss2021\nand import with the IDE of your choice:\n\nInstructions for Eclipse\nInstructions for IntelliJ\n\nRun Your First ImageJ Program\nYou should now be able to execute the file src/main/java/exercises/Exercise00.java.\n\n\nThe following code is opening the ImageJ main window and exits the running program when the window is closed.\n\npublic class Exercise00 {\n\n    public static void main(String[] args) {\n        (new ij.ImageJ()).exitWhenQuitting(true)\n\n    }\n}\n\nWhen you use IntelliJ, you can just open the file Exercise00 and then click on the green\narrow:\n\nDebugging\nEverything here should just be a recap of what you know about debugging from AuD. Feel free to skip if you know how debugging works!\nTo do the exercises during this semester, you will often need to use the debug mode to find errors in your code.\nPlease make sure that you know how to run your program in debug mode, since you might not have seen it before (Shame on you AuD!).\nPlease always try to debug an issue first before you ask a tutor for help!\nTo debug an application just click on the bug symbol! You can try this with file src/main/java/exercises/DebugExample.java.\n\n\nAlso set some breakpoints (where the program should stop) by clicking on the space left to the line numbers.\n\n\nA Breakpoint. The program will stop here in debug mode.\n\nYou should also know how to make the debugger stop on an exception (to see what's wrong when the program is crashing).\nThere is a bug in program! When you run it, you should see a crash:\n\nIntelliJ suggests to create a break point. Click Create breakpoint!\n\nConfirm with Done.\n\nWhen you run the program again in debug mode it will stop at the line where the error happened.\nThis will help you to find out what is wrong at that point.\nAlso Eclispe will stop automatically and indicate what the problem is (if it doesn't please upgrade to the newest version of Eclispe):\n\nVery useful is also the calculator symbol, that let's you evaluate expressions.\nIf you don't know, what is wrong in a line, you can tell the calculator to the termine the value of a or a.x to determine which one\nis null (or of very.complicated.expression[2] when things get more complicated and a variable is not shown in the list below).\n\nIn Eclipse, the calculator are a pair of glasses (make sure you are in debug perspective, menu: Window &gt; Perspective &gt; Open Perspective &gt; Debug).\n\nNow you should be well prepared for the actual exercises!\n"
}
,

{
"title": "Project Work 6 – Iterative Reconstruction and Conclusion",
"url": "https://mt2-erlangen.github.io/archive/2020/reconstruction/",
"body": "Iterative Reconstruction\nUsing backprojection, we could achieve a blurry reconstruction result.\nThe Filtered Backprojection algorithm solves this problem by applying a filtering step before backprojection.\nFor the project work, we will take a different approach.\nIn the last section, you measured the error between your reconstruction and the ground truth volume.\nHowever, this is only possible when doing a simulation and not when reconstructing an unknown real object.\nWhat we can do instead is meassuring the error in the projection domain by simply projecting the reconstruction!\nImplement the following method to use with our reconstructionProjector:\n\n    // In mt/Projector.java\n    public void reconstructIteratively(Image meassuredProjection, int sliceIdx, int numIterations)\n\nIt should\n\ncall projectSlice on volume to obtain a projection of our reconstruction\ncalculate an error image subtracting singogram.getSlice(sliceIdx) from meassuredProjection\nreplace the current slice of singogram by our error image\ncall backprojectSlice with the current sliceIdx\nrepeat all this for numIterations iterations\n\n\nSo we're now doing an reconstruction of the error sinogram and adding it to our blurry image.\nDoes this reduce our error?\nOur reconstruction algorithm is now finished. But it operates only on 2-d slices.\nCreate 3-d versions of projectSlice, backprojectSlice and reconstructIteratively:\n\n    public void project()\n    public void backproject()\n    public void reconstructIteratively(Volume measuredProjections, int numIterations)\n\nAll they should do is calling their 2-d version for each slice.\nYou should now be able to reconstruct volumes.\nHint: You can use the following construct instead of a for-loop to enable multi-threaded calculation.\n\n    // You have to replace `var` by `java.util.concurrent.atomic.AtomicInteger` when using Java 1.8\n    var progress = new java.util.concurrent.atomic.AtomicInteger(0);\n    IntStream.range(0, sinogram.depth()).parallel().forEach(z -&gt; {\n        System.out.println(&quot;Progess: &quot; + (int) (progress.incrementAndGet() * 100.0 / (double) sinogram.depth()) + &quot; %&quot;);\n        //Do stuff here for slice z\n        ...\n    });\nProject Report\nFor the project, describe how your iterative reconstruction algorithm works. You should not mention implementation details\nlike variable or function names. Compare it with the Filtered Backprojection algorithm! It's not necessary to explain \nFiltered Backprojection Algorithm in detail. Just highlight the main difference.\nTest your reconstruction algorithm on a slice of a CT reconstruction of the Cancer Imaging Archive.\nMeasure the error of the reconstructed slices after each iteration (so call reconstructIteratively with numIterations == 1).\nInclude a figure showing this error in dependence of the iteration number in the project report.\nInclude images comparing ground truth, the backprojected slice and the result after a few iterations.\nComment on the error and the images in your text.\nDoes the result of the iterative reconstruction look better than solely using backprojection?\nThis part of the project report should be no longer than 1.5 pages.\nConclusion\nIn the last part, summarize want you have implemented and explained in your project report.\nReview the shortcommings of your simplified approach and how they could be mitigated in future.\nDraw a conclusion on your work!\nThis part of the project work should be about a quarter page long and should contain no images.\nSubmission\nSubmit your project report as a PDF and your entire project folder of your code until August 16 23:55h.\nYour project must compile as a whole!\nMake sure that you had a last look at our checklist.\nEvaluation\nWe hope you had a fun project work!\nYou can help us to improve the instructions for next year!\nPrevious section\n"
},

{
"title": "Project Work 5 – Backprojection",
"url": "https://mt2-erlangen.github.io/archive/2020/backprojection/",
"body": "Backprojection\nIf we have a look at the sinogram values corresponding to one detector position we get some information about the projected object.\nFor instance, we can see the profile of the projected circle in the following image.\n\nHowever, if we have no access to the original volume slice we can not tell anything about the distance of the object to the detector.\nAll the following situations would generate the same projection!\n\nSo apparently, we get some information in the direction of the detector plane, but all information orthogonal to the detector plane\nis lost.\nSo one thing that we can do if we want to perform a reconstruction from the sinogram is to take the information in direction of the detector plane\nand uniormly smear it into the direction orthogonal to the detector plane in a range where we assume the object is located.\nWe call this process backprojection.\n\n\n    \n\n\n    The backprojection smears the value of the projection uniformly over the paths of the rays\n\n\nUse the following method, that is calculating the value that we want to smear back.\n\n    // in mt.Projector\n    public float backprojectRay(mt.Image sinogramSlice, int angleIdx, float s) {\n        sinogramSlice.setOrigin(0.f, -sinogram.physicalHeight * 0.5f);\n        return sinogramSlice.interpolatedAt(angleIdx * sinogram.spacing, s) // * sinogram.spacing is necessary because spacing is not valid for our angle indices (actually each coordinate should have their own spacing. That&#39;s the revenge for us being lazy.).\n                / (volume.physicalWidth() * Math.sqrt(2)) // we guess that this is the size of our object, diagonal of our slice\n                / sinogramSlice.width()  // we will backproject for each angle. We can take the mean of all angle position that we have here.\n                ;\n    }\n\nUse this method in backprojectSlice to backproject for each pixel x, y a horizontal line of the sinogram (all possible angles).\n\n    // in mt.Projector\n    public void backprojectSlice(int sliceIdx)\n    // A helper method\n    public void backprojectSlice(int sliceIdx, int angleIdx)\n\nTo do this \n\nCreate a loop over all angleIdx\n\nCall the helper method for all angle indices (there are sinogram.width angles)\n\n\nIn public void backprojectSlice(int sliceIdx, int angleIdx)\n\nGet the slice with index sliceIdx\nLoop over all x, y of this image\nCalculate the physical coordinates from the integers x and y (times spacing plus origin!)\nCalculate the actual angle theta from the angleIdx\nCalculate s from the physical coordinate.\n\ns is the physical distance of the point $\\vec{x}$ from the ray through the origin at angle theta.\nCan you write down the line equation for this line?\nCan you use the line equation to calculate the distance of $\\vec{x}$ an the line through the origin?\n\n\nCall backprojectRay with angleIdx and s\nAdd this result of backprojectRay to current value at position x, y and save the sum at that position\n\n\n\n\n\n\nReconstruction\nNext, we want to try out whether we can use our backprojection to reconstruct a volume.\nWhenever we want to test whether a method works, we need something to compare it with.\nThe best possible result, the &quot;true&quot; values, is usally called ground truth.\nWe can use one of the reconstructions that we downloaded from the Cancer Imaging Archive as a ground truth volume.\nThe best possible result for our reconstruction is to come as close as possible to the original (ground truth) volume.\nCreate a file src/main/java/project/GroundTruthReconstruction.java.\n\n// Your name &lt;your idm&gt;\npackage project;\n\nimport mt.Projector;\nimport mt.Volume;\n\nclass GroundTruthReconstruction {\n\n    public static void main(String[] args) {\n        (new ij.ImageJ()).exitWhenQuitting(true);\n\n    }\n}\n\nIt's important that we never mix up the ground truth with the results of our algorithm.\nCreate therefore an instance of Projector that will have the task to simulate projections.\nYou can call it groundTruthProjector.\nOpen a test volume and create an empty (all pixels 0) sinogram. They are needed to call the constructor of Projector.\nCall groundTruthProjector.projectSlice with an arbiray slice index.\n\nCreate an empty volume (all pixels 0)  with the same dimensions as the ground truth volume and a copy of groundTruthProjector.sinogram().\nYou can add the following method to mt.Volume to create copies.\n\n    // in mt/Volume.java\n    public Volume clone(String name) {\n        Volume result = new Volume(width(), height(), depth(), name);\n        IntStream.range(0, depth()).forEach(z-&gt; result.getSlice(z).setBuffer(Arrays.copyOf(slices[z].buffer(), slices[z].buffer().length)));\n        return result;\n    }\n\nCreate a new projector reconstructionProjector with the empty volume and the copy of our sinogram.\nUse backprojectSlice(...) to create your first reconstruction of a slice.\nA good way to test your implementation is to incremently apply more and more backprojections on your reconstruction.\nWhen you calculated the sinogram for SLICE_IDX you can use\n\n// in project.GroundTruthReconstruction.java\n\n// Choose the slice in the middle. Hopefully showing something interesting.\nfinal int SLICE_IDX = ????; // &lt; Use a index for which you already calculated `projectSlice`\n\nfor (int i = 0; i&lt; projector.numAngles(); i++ ) {\n    try {\n        TimeUnit.MILLISECONDS.sleep(500);\n    } catch (InterruptedException e) {\n        e.printStackTrace();\n    }\n    projector.backprojectSlice(SLICE_IDX, i);\n    projector.volume().getSlice(SLICE_IDX).show();\n\n    //// Optionally save the intermediate results to a file:\n    //DisplayUtils.saveImage(projector.volume().getSlice(SLICE_IDX), &quot;/media/dos/shepp_9_&quot;+i+&quot;.png&quot;);\n}\n\nThis will wait 500ms between each backprojection. Do your rays meet at the right points? Use a simple test image with\nonly a single white circle if not. This should help you debug the issue.\n\n    \n         \n            \n            \n             \n        \n         \n            \n            \n             \n        \n    \n    \n        Backprojection using 9 views\n        Backprojection using 100 views\n    \n\nProject Report\nFor the project report, you should briefly describe your backprojection reconstruction algorithm.\n\nDescribe your implementation, create at least one figure supporting your explanations.\nYou should never mention implementation details like for-loops or variable names, but important parameters like the number\nof projection angles you used\nTest your reconstruction algorithm\n\nusing a simple test image like a white circle or square\nusing a CT reconstruction that you downloaded . Cite the data source!\n\n\nHow do images look like? If they are blurry, what is the reason for that.\nShow the images in your project report.\nMention in one sentence how the Filtered Backprojection algorithm tries to solve that problem.\nHow big are your errors in comparison to the ground truth? If you are using a measure like the Mean Squared Error give\na formula defining it.\n\nThe content for this section should be about one page long. \nPrevious section\nNext section\n"
},

{
"title": "Project Work 4 – Sinogram",
"url": "https://mt2-erlangen.github.io/archive/2020/sinogram/",
"body": "Sinogram\n\nNow, you should be able to generate sinograms from volume slice.\nGenerate two sinograms from two volume slices:\n\n\nOne sinogram from a simple test image. You can use for instance a white circle as I was doing in the last section.\n\n\nOne sinogram from a real CT reconstruction. You should cite the source of that image. The Cancer Imaging Archive even\nexplains you how to do that.\n\n\nShow both the volume slices and the sinograms.\nExplain to the reader what they are seeing. What is the radon transform?\nCan the radon transform be inverted?\n\n\nDo the sinograms contain some kind of symmetry? What is the reason for that?\nDo we really need a 360° degree scan?\n\n\n\n\nThis section should not be longer than one page.\nPrevious section\nNext section\n"
},

{
"title": "Project Work 3 – Projection",
"url": "https://mt2-erlangen.github.io/archive/2020/projection/",
"body": "Projections\nTo understand how we can reconstruct a volume from X-ray images, we will first go through the process of how these X-ray images\nwere acquired from a physical volume.\nIn your project report you should...\n\nexplain the reader the physical process of X-ray attenuation and its material dependance.\nWhat materials in the human body attenuate more X-rays than others?\nHow is this represented in a CT reconstruction? Or in other words: what quantity does a CT reconstruction actually show?\nWhich kind of tissues appear therefore lighter and which darker?\nexplain the fundamental theorem hat describes this process (X-ray attenuation). Give a formula!\nExplain all the symbols that you use in the formula.\nprove your explanations with references, also provide the source of the formula.\n\nIn this project work, we will make some simplifying assumptions on the acquisition geometry.\nI made a drawing of the path of a single X-ray through a slice of our volume.\nSince this ray crosses the origin of our coordinate system we call it the principal ray.\n\nWhat are the coordinates $\\vec{x}_{P}$ of a point $P$ on the line of the principal ray in dependency of the angle $\\theta$ ($\\alpha$ in drawing) and the distance\nfrom origin $r$?\nIn reality, not all X-rays cross the coordinate origin. \nWhat are the coordinates $\\vec{x}_{P'}$ of a point $P'$ that is on a ray that hits the detector at coordinate $s$ in depedency of $r$ and $\\theta$?\nWe assume parallel rays.\nHint: What vector do you have to add to $P$ to get to $P'$?\n\nUnfortunally, the figure was written on paper and you shouldn't use hand drawn figures in the project report (as you can see they look ugly).\nPlease create one or two plots on the computer that are explaining your derived the ray equations to the reader of the project\nreport. Decide which information is important for the reader to understand your text.\n\nHow does the described situation differ from the actual acquisition geometry of modern CT scanners?\nWhat are the reasons for that? Could our simplified situation be implemented in reality?\n\n\n\nAfter Implementation: Describe briefly your implementation of the projection.\nDo not refer any Java classes or variable names!\nGive a formula for how you calculated the different projection angles.\nGive a formula for how you calculated the projection result for each ray.\nWhat physical effects were neglected in our simulation but are present in reality?\nName at least three non-idealities of real systems.\n\nThis part of the project work should be not longer than 1.5 pages.\nAfter some remarks from you: 2 pages are also ok..\nImplementation\nWe already have an volume class which can store the stack of image slices. Additionally, we also want\nto store the projection images (referred as sinograms) for these stack of image slices. For this create\ncreate a class mt.Projector in a file src/main/java/mt/Projector.java, which can hold both volume slices\nand the sinograms.\n\n// Your name here &lt;your idm&gt;\npackage mt;\n\nimport java.util.stream.IntStream;\n\npublic class Projector {\n    // Our volume\n    private mt.Volume volume;\n    // Our sinogram\n    private mt.Volume sinogram;\n\n}\n\nImlement a constructor for this class.\nIt should call this.volume.centerOrigin() and set the origin of each sinogram slice to 0.0f, -sinogram.physicalHeight() * 0.5f so we use the same coordinate\nsystems as in our drawings (it might be handy to set the origin of sinogram to 0.0f, -sinogram.physicalHeight() * 0.5f, -sinogram.physicalDepth() * 0.5f, requires a Volume.setOrigin method)\n\n    public Projector(mt.Volume projectionVolume, mt.Volume sinogram) {\n        ... // Implementation here\n        assert sinogram.depth() == volume.depth() : &quot;Should have same amount of slices&quot;;\n    }\n\nConstructor and Setters/Getters:\n\n    public void setSinogram(Volume sinogram)\n    public Volume sinogram()\n\n    public void setVolume(Volume volume)\n    public Volume volume()\n\n    public int numAngles() // == sinogram.width()\n\nWe assume that we aquire $N$ projections at $N$ different angles $\\theta$.\nAll angles should have the same distance from each other and divide $2\\cdot \\pi$ in $N$ equal parts (we always use radians for angles).\nImplement a method which computes angle value of $n^{th}$ angle index. We want to use the method such that at $n=0$ our angle value should return $\\theta=0$, at $n=1$ returns $\\theta= \\frac{2\\cdot \\pi}{N}$, and so on. Think of a general formula to compute the $n^{th}$ angle and describe it briefly in the description of your implmentation.\nUse this formula to implement the following method:\n\n    // In mt.Projector\n    public float getNthAngle(int angleIdx)\n\nNow, recall the formula you derived for the position of point $P'$ in the previous section.\nWe could directly use those coordinates $\\vec{x}$ to calculate the integral in Lambert-Beer's law for a ray with angle $\\theta$ and shift $s$ over a slice $\\mu$ on our computers:\n$$ I_{\\textrm{mono}} = I_{0} \\cdot  \\exp\\left(-\\intop\\mu\\left(\\vec{x}\\right)\\textrm{d}\\vec{x}\\right) = I_{0} \\cdot  \\exp\\left(-\\intop_{-R}^{R}\\mu\\left(r,\\theta, s\\right)\\textrm{d}r\\right)$$\n$R$ is the radius of the circle circumscribing our rectangular slice. You can see it in the drawing.\nThe path integral goes along the path marked in yellow in the drawings.\nWe are only interested in the value of the line integral\n$$ P(\\theta, s) = \\intop_{-R}^{R}\\mu\\left(r, s, \\theta\\right)\\textrm{d}r $$\nand we have to replace the integral by a sum (computers cannot calculate integrals directly)\n$$ P(\\theta, s) = \\sum_{r=-R}^{R}\\mu\\left(r,\\theta, s\\right) \\cdot \\mathtt{spacing}$$\nCalculate this sum for a fixed $s$ and $\\theta$ on a slice of our volume!\nYou can use volumeSlice.interpolatedAt(x,y) to deterime $\\mu(\\vec{x})$ and access values of our slice.\n\n    // in mt.Projector\n    public float projectRay(mt.Image volumeSlice, float s, float theta)\n\nWe have now calculated one value of one of the gray rays on our slice which translates to one point in our sinogram.\n\nNext we want to call this function for every ray and every pixel of our sinogram in the following method:\n\n    // in mt.Projector\n    public void projectSlice(int sliceIdx) {\n\nTo do that ...\n\n\nGet the slice sliceIdx from this.volume using getSlice\n\nThis is a slice of our volume with coordinates $x$ and $y$.\n$x$ runs from left to right\n$y$ runs from top to bottom\n\n\n\nGet the sinogram for that slice sliceIdx from this.sinogramm using getSlice\n\nThis is a slice of our sinogram with physical coordinates $s$ and $\\theta$.\n$\\theta$ runs from left to right\n$s$ runs from top to bottom\n\n\n\nIterate over each pixel of the sinogram. I would use angleIdx, sIndex  as a loop variables.\n\nCalculate the actual value of s from sIndex.\nCalculate theata from angleIndex by calling the function getNthAngle\nCall projectRay with s and theta\nSave the result to sinogram at positions angleIndex and sIndex\n\n\n\nHint Computing s from sIndex is just using the physical coordinates and shifting the origin of $s$ axis in\nthe sinogram to the center.\nThis can be done by muliplying sIndex with sinogram.spacing() (pixel size of the detector) and adding\nsinogram.origin()[1] (== -sinogram.physicalHeight() * 0.5f).\nWe recommend you to test your algorithm using a simple image.\nChoose a good size for the sinogram to capture the whole image (e.g. height == volume.height).\nFor simplicity, you do not need to change the spacing of the volume or the sinogram.\n\n \n    \n    \n\n \n    Simple test slice\n    Sinogram of that slice\n\n\nI used a high number of 500 angles to get a near square image.\nWhen you are using less angles the width of your sinogram will be smaller.\nUse less angles to compute the results faster.\nYou may also apply projectSlice on all slices and display the sinogram.\nCtrl+Shift+H should reveal a rotating torso when using one the Cancer Archive scans:\n\n  \n \nOr of the test image above\n\n  \n \nPrevious section\nNext section\n"
},

{
"title": "Project Work 2 – Volumes",
"url": "https://mt2-erlangen.github.io/archive/2020/volume/",
"body": "Getting started\nImportant: You have to work alone on your project work. No team partners allowed anymore 😔!\nCT reconstruction treats the problem of recovering a three-dimensional volume from a set of X-ray images.\nSo we will need two classes that represent our volume and our stack of X-ray projections.\nIt turns out that we can interpret our projections and our volume just as a list of 2-d images.\n\n\n\n\n\nA volume: very much just multiple images stacked one over another\n\n\nCreate a class mt.Volume\n\n// Your name &lt;your idm&gt;\n// No team partner... So sad 😢!\n\npackage mt;\n\nimport java.util.Arrays;\nimport java.util.stream.IntStream;\n\npublic class Volume {\n    // Here we store our images\n    protected mt.Image[] slices;\n\n    // Dimensions of our volume\n    protected int width, height, depth;\n\n    // Spacing and origin like for mt.Image\n    protected float spacing = 1.f; // spacing is now our voxel size\n    protected float[] origin = new float[]{0, 0, 0}; // position of the top-left-bottom corner\n\n    // A name for the volume\n    protected String name;\n\n}\n\nCreate a constructor. Remember: width, height, depth, name must be set and slices must be created as an array.\nWe need depth images of size width $\\times$ height for the slices.\n\n    public Volume(int width, int height, int depth, String name)\n\nGetters/setters...\n\n    public int width()\n    public int height()\n    public int depth()\n    public float physicalWidth() // width * spacing()\n    public float physicalHeight() // height * spacing()\n    public float physicalDepth() // depth * spacing()\n\n    public mt.Image getSlice(int z) \n    public void setSlice(int z, mt.Image slice)\n\n    public float spacing()\n    public void setSpacing(float spacing) // should also set spacing also for all slices!\n    public String name()\n    public float[] origin()\n\n    // should set origin to (-0.5 physicalWidth, -0.5 physicalHeight, -0.5 physicalDepth) and call centerOrigin on each slice\n    public void centerOrigin()\n\nNow comes the interesting part: visualize the volume!\nYou will need to update src/main/java/lme/DisplayUtils.java file and use the following command to visualize the volume.\n\n    public void show() {\n        lme.DisplayUtils.showVolume(this);\n    }\n\nYou can download a volume from the Cancer Imaging Archive.\nUse one of the following links (it does not matter which CT volume you use).\n\nVolume 1\nVolume 2\nVolume 3\n\nUnzip the folder and drag the whole folder onto a running ImageJ, e.g. by the following code snippet in a file src/main/java/project/Playground.java.\n(if you have problems unzipping the files you might try the official downloader from the website. You need their downloader to open the *.tcia files).\n\n// This file is only for you to experiment. We will not correct it.\n\npackage project;\n\nimport mt.Volume;\n\nclass Playground {\n\n    public static void main(String[] args) {\n        // Starts ImageJ\n        (new ij.ImageJ()).exitWhenQuitting(true);\n\n        // You can now use drag &amp; drop to convert the downloaded folder into a *.tif file\n        \n    }\n\n}\n\n\nSave it the opened DICOM as a *.tif file (File &gt; Save As &gt; Tiff...).\nThere are more smaller test volumes on studOn.\n\n  \n \n\n\n\nOpen the saved tiff file in the main of a file src/main/java/project/Playground.java:\n\n// This file is only for you to experiment. We will not correct it.\n\npackage project;\n\nimport mt.Volume;\n\nclass Playground {\n\n    public static void main(String[] args) {\n        (new ij.ImageJ()).exitWhenQuitting(true);\n        \n        Volume groundTruth = DisplayUtils.openVolume(&quot;path/to/file.tif&quot;);\n        groundTruth.show();\n        \n    }\n\n}\n\n\nYou can now scroll through the different slices.\nvia GIPHY\nHere a short summary of handy functions of ImageJ when working with CT images.\n\nCtrl+Shift+C: Brightness and Contrast\nCtrl+Shift+H: Orthogonal Views (view volume from three sides)\nAfter selecting a line: Ctrl+K Line Plot\nCtrl+I: Get patient information of a DICOM\nLook at a 3-d rendering with ClearVolume\n\nPrevious: Introduction \nNext: Forward Projection\n"
},

{
"title": "Project Work 1 – Introduction",
"url": "https://mt2-erlangen.github.io/archive/2020/introduction/",
"body": "Contents\n\nIntroduction Tafelübung 9. Juni\nVolumes\nProjection Tafelübung 16. Juni\nSinogram\nBackprojection and Reconstruction Tafelübung 23. Juni\nIterative Reconstruction and Conclusion\n\nIntroduction\nDuring this semester we will learn how computer tomography (CT) reconstruction algorithms work.\nYour first task is to find out more about CT and write an introduction for your project report.\n\nFind an informative title for your project report. &quot;Project Report&quot; and &quot;Introduction&quot; are not good titles.\nWhat is computer tomography?\nWhat is the problem it tries to solve? When and how was it first introduced?\nWhat kind of electromagnetic radition is used to aquire the images?\nHow did modern CT devices improve over their predecessors? What is the typical spatial resolution of a state-of-the-art CT scanner?\nWhat are advantages and disadvantages of CT in comparison with other modalities. Include at least two advatages and\ntwo disadvantages.\nGive a short overview of the contents of the following sections of your project report.\nProof all your statements with references. You should use at least four distinct sources in your introduction that are\nnot webpages.\n\nThe introduction should not be longer than one page and but at least half a page. \nYour introduction and conclusion should not contain any images.\nPlease have a look on our checklist for a good project report.\n\n\nNext task\n"
},

{
"title": "Exercise 6",
"url": "https://mt2-erlangen.github.io/archive/2020/exercise-6/",
"body": "Submission deadline: 29.06.20 23:55h\nIn the last exercise, we want to have a look at edge detection and segmentation.\nEdge Detection\n 7 Points\nOpen a test image in a new file src/main/java/exercise/Exercise06.java.\n\n// Your name\n// Team parnter name\npackage exercises;\n\nimport lme.DisplayUtils;\nimport mt.LinearImageFilter;\n\npublic class Exercise06 {\n    public static void main(String[] args) {\n\t(new ij.ImageJ()).exitWhenQuitting(true);\n\tmt.Image cells = lme.DisplayUtils.openImageFromInternet(&quot;https://upload.wikimedia.org/wikipedia/commons/8/86/Emphysema_H_and_E.jpg&quot;, &quot;.jpg&quot;);\n\n    }\n}\n\nWe will use the Sobel Filter, to estimate the gradient of the image.\nThe Sobel Filter uses two filter kernels. One to estimate the x-component of the gradient and one for the y-component.\n\nCreate two LinearImageFilters with those coeffients. You can use filterX.setBuffer(new float[]{...})\nor setAtIndex to do that.\nFilter the original image with both of them!\n\n    \n\t\n\t\n    \n    \n\tX component of gradient $\\delta_x$\n\tY component of gradient $\\delta_y$\n    \n\nYou should now have two intermediate results that can be interpreted as the x-component $\\delta_x$\nand y-component $\\delta_y$of the estimated gradient for each pixel.\nUse those two images to calculate the norm of the gradient for each pixel!\n$$ \\left|\\left| \\nabla I \\right|\\right| =\\left|\\left| \\left(\\delta_x,\\ \\delta_x \\right) \\right|\\right| = \\sqrt{ \\delta_x^2 + \\delta_y^2}$$\n\nFind a good threshold and set all gradient magnitude values to zero that are below this values and all other to 1.f to\nobtain an image like this with a clear segmentation in edge pixels and non-edge pixels.\n\nSegmentation\n 3 Points\n\n Source: https://commons.wikimedia.org/wiki/File:Emphysema_H_and_E.jpg (cc-by-2.0)\nFor histologic examinations colored subtances called stains are used to enhance the constrast\nof different portions of the tissue.\nUse a suitable threshold to segment the individual sites with high contrast (0 background, 1 contrasted cells).\nYou can use the following method to overlay your segmentation with the original image.\n\n    // In lme.DisplayUtils\n    public static void showSegmentedCells(mt.Image original, mt.Image segmented) \n    // You may also try `showSegmentedCells(cells, segmentation, true);` with the newest version of DisplayUtils\n\n\nImproving your Segmentation\nThis is optional and not required for the exercise.\nYou might want to go directly to the evaluation of this year's exercises:\nhttps://forms.gle/2pbmuWtmeTtaVcKL7\nYou may notice that by just choosing a threshold you may not be able to separate each individual structure.\n\nYou can try out some operations from the menu Process &gt; Binary while you have your 0/1 segmentation focused.\nYou have to convert to 8-bit first. E.g.\n\n\n\nImage &gt; Type &gt; 8-bit\nProcess &gt; Binary &gt; Watershed\n\n\nOr &quot;click&quot; on menu items in your program code.\n\n    segmentation.show();\n    IJ.run(&quot;8-bit&quot;);\n    IJ.run(&quot;Watershed&quot;);\n    DisplayUtils.showSegmentedCells(cells, segmentation);\n\n\nEvaluation\nWe redesigned the exercises from scratch for this semester.\nTherefore, some of the exercises might have been difficult to understand or too much work. \nWe are glad for your feedback to help future semesters' students😊:\nhttps://forms.gle/2pbmuWtmeTtaVcKL7\n\n\n\n\n\n"
},

{
"title": "Exercise 5",
"url": "https://mt2-erlangen.github.io/archive/2020/exercise-5/",
"body": "Submission\nSubmission deadline: 08.06.20 23:55h\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nQuanitfying Errors\n3 Points\nIn Exercise03, we have seen that we can use linear low-pass filters, like the Gauss filter, to reduce \nthe amount of noise in images. Let's test that!\nAdd two static methods to the Image class:\n\npublic static float meanSquaredError(Image a, Image b);\npublic static float psnr(Image a, Image b, float maxValue); // maxValue is 255 for PNG images\n\n$$ \\mathrm{MSE}_{ab}=  \\frac{1}{M} \\sum _{i=0}^{M} \\left(a_i - b_i\\right)^2 $$\n$$        \\mathrm{PSNR_{ab}} = 20\\cdot  \\log_{10}(\\mathtt{maxPossibleValue}) - 10\\cdot \\log_{10}(\\mathrm{MSE}_{ab}) $$\n\nStatic also means that you will use them like float mse = Image.meanSquaredError(imageA, imageB);.\nOpen a test image and add some noise using addNoise in exercise.Exercise05 (src/main/java/exercise/Exercise05).\n\n    (new ij.ImageJ()).exitWhenQuitting(true);\n    Image original = lme.DisplayUtils.openImageFromInternet(&quot;https://mt2-erlangen.github.io/shepp_logan.png&quot;, &quot;.png&quot;);\n    original.setName(&quot;Original&quot;);\n    \n    Image noise = new Image(original.width(), original.height(), &quot;Noise&quot;);\n    noise.addNoise(0.f, 10.f);\n\n    Image noisyImage = original.minus(noise); // You might also implement your own `plus` ;-)\n\nApply a Gauss filter (choose a good filterSize and sigma) on the noise image and compare the result with the original image.\nCan the error be reduced in comparision to the unfiltered noisy image? Also take a look on the error images that you can\ncalculate using your minus method of the class Image.\n\nHint: You can use a for-loop to try out different values for sigma.\nHint: You do not need to submit written answers to the questions in the text. Just do the correponding experiments!\n\nNon-Linear Filters\n3 Points\nA quality criterion for medical images are sharp edges.\nHowever, though the Gauss filter reduces the noise it also blurs out those edges.\nIn this exercise, we try to mitigate that problem using non-linear filters.\nNon-linear filters calculate similar to a convolution each pixel value in the output from a neighborhood of the\ninput image. Remember the sliding window from exercise 3? Non-linear filters do exactly the same.\n\nSource: https://github.com/vdumoulin/conv_arithmetic\nCreate a class mt.NonLinearFilter in the file src/main/java/mt/NonLinearFilter.java:\n\n// Your name here &lt;your idm&gt;\n// Your team partner here &lt;partner&#39;s idm&gt;\npackage mt;\n\nimport lme.WeightingFunction2d;\nimport lme.NeighborhoodReductionFunction;\n\npublic class NonLinearFilter implements ImageFilter {\n\n    // Name of the filter\n    protected String name; \n    // Size of the neighborhood, 3 would mean a 3x3 neighborhood\n    protected int filterSize;\n    // Calculates a weight for each neighbor\n    protected WeightingFunction2d weightingFunction = (centerValue,neighborValue,x,y) -&gt; 1.f;\n    // Calculates output value from neighbors and weights\n    protected lme.NeighborhoodReductionFunction reductionFunction;\n\n    public NonLinearFilter(String name, int filterSize) {\n        this.filterSize = filterSize;\n        this.name = name;\n    }\n\n    @Override\n    public String name() {\n        return name;\n    }\n}\n\nAs you can see, NonLinearFilter uses two interfaces. You can copy them into your src/main/java/lme/ folder.\n\n// in file `src/main/java/lme/WeightingFunction2d.java`\npackage lme;\n\n@FunctionalInterface // Does nothing. But Eclipse is happier when it&#39;s there.\npublic interface WeightingFunction2d {\n    // Assigns  a neighbor (shiftX, shiftY) a weight depending on its value and the value of the pixel in the middle of the neighborhood\n    float getWeight(float centerValue, float neighborValue, int shiftX, int shiftY);\n}\n\nand\n\n// in file `src/main/java/lme/NeighborhoodReductionFunction.java`\npackage lme;\n\n@FunctionalInterface\npublic interface NeighborhoodReductionFunction {\n    // Calculates the output pixels from the values of the neighborhood pixels and their weight\n    float reduce(float[] values, float[] weights);\n}\n\nImplement the method apply for NonLinearFilter.\n\n    @Override\n    public void apply(Image input, Image result)\n\nThe method should calculate each output pixel from a neighborhood. So\n\nCreate an array to hold the values of the neighborhood pixels. How many neighborhood pixels are there?\nLoop over each output pixel\n\nFill the array of neighborhood pixels with values from the input image (needs two inner loops)\nUse this.reductionFunction.reduce to determine the value of the output pixel. You can use null for the second parameter for now (we will implement weights later).\nSave the value to the output image (using setAtIndex).\n\n\n\nOverall, the method should look very similar to your LinearImageFilter.apply method.\nTo test your method, implement a MedianFilter in a file src/main/mt/MedianFilter.java as a subclass of NonLinearFilter.\n\n// Your name here\n// Team partner&#39;s name here\npackage mt;\n\nimport java.util.Arrays;\n\npublic class MedianFilter extends NonLinearFilter {\n\tpublic MedianFilter(int filterSize) {\n            // TODO:\n            super(...);\n            reductionFunction = ...;\n\t}\n}\n\nThe MedianFilter is a LinearImageFilter with\nreductionFunction (values, weights) -&gt; { Arrays.sort(values); return values[values.length / 2]; }\n(it sorts the values and takes the one in the middle).\nAll you need to do is to call the super constructor and set reductionFunction.\nDoes the median filter also reduce the noise in the image?\nBilateral Filter\n2 Points\nNext, we will implement the BilateralFilter.\n\npackage mt;\n\npublic class BilateralFilter extends NonLinearFilter {\n    GaussFilter2d gaussFilter;\n\n    public BilateralFilter(int filterSize, float spatialSigma, float valueSigma){\n        ...\n    }\n}\n\nThe bilateral assign a weight to each neightborhood pixel.\nSo modify your NonLinearFilter.apply method that it also creates a weights array and uses weightingFunction.getWeight to\nfill it. reductionFunction should now also be called with the weights array.\nThe bilateral has to parameters $\\sigma_{\\text{value}}$ and $\\sigma_{\\text{spatial}}$.\nFor large values of $\\sigma_{\\text{spatial}}$ the bilateral filter behaves like a Gauss filter.\nInitialize gaussFilter in the constructor. Set weightingFunction so that the weights $w_s$ of the Gauss filter are returned.\nSet reductionFunction. It should multiply each of the values with its weight and then sum the results up.\nYour BilateralFilter should now behave like a Gauss filter. Does it pass the test in GaussFilter2dTests when you\nuse BilateralFilter instead of GaussFilter2d?\nEdge-Preserving Filtering\n2 Points\nTo make our bilateral filter edge preserving, we have to use also $\\sigma_{\\text{value}}$.\nThe value weight $w_v$ is calculated as follows\n$$ w_v = \\exp\\left(-\\frac{\\left(\\mathtt{centerValue}-\\mathtt{value}\\right)^2}{2 \\sigma_{\\text{value}}^2}\\right) $$\nJust multiply with this value $w_v$ in weightingFunction. The total weight of a pixel will then be $w_v \\cdot w_s$.\nNow we have the problem that our weights will no longer add up to one! To solve this problem divide by the sum of weights\nin the reductionFunction.\nCan you reduce the error even more using the bilateral filter? My results look like this.\n\n  \n    \n    \n    \n    \n  \n  \n    Original\n    Noisy\n    Gauss filtered\n    Bilateral filtered\n  \n  \n    \n    \n    \n    \n  \n  \n    \n    Error Unfiltered\n    Error Gauss\n    Error Bilateral\n  \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
},

{
"title": "Exercise 4",
"url": "https://mt2-erlangen.github.io/archive/2020/exercise-4/",
"body": "Submission\nSubmission deadline: 01.06.20 23:55h\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nImage Transformations\nIn the previous exercises, we built a Signal and Image class for performing basic operations on the \ninput data. We also implemented various filters to process the data and remove noise. \nIn this exercise we will build on top of the image class and implement methods for performing image transformations.\nIn many medical applications there is a need to align two images so that we \ncan combine the information between the images. This can be due to the images coming from \ndifferent modalities like (CT and MRI) or in scenarios were you have an patient data at from \ndifferent time (before and after an surgery)  and you want to compare between these two images. \nIn all these scenarios we use image registration bring the different images together.\nIn the below image, two x-ray views (1) and (2) are fused together to obtain the combined view(3)\nwhich produces more information for diagnosis. This is achieved using image registration between view(1) and view\n \nImage Source: Hawkes, David J., et al. &quot;Registration and display of the combined bone scan and \nradiograph in the diagnosis and management of wrist injuries.&quot; European journal of nuclear medicine \n18.9 (1991): 752-756. \nOne of the crucial components of image registration is image transformations.\nIn this exercise we will implement basic image transformations. Additionally, we need to implement an \ninterpolation method to find out the image intensity values at the transformed coordinates. \n\nOverview of tasks\n\nWe will implement the following tasks for this exercise.\n\nHelper functions (a. Image origin, b. Interpolation)\nImage Transformation (a. Translation, b. Rotation, c. Scaling)\n\nWe introduce the basic theory about image transformations in theoretical background section.\nPlease read the theory before proceeding since we don't re-introduce everything in the task description. \nTask Description\n\nWe provide the main method for the task with an interactive ImageJ plug-in in the files\nsrc/main/java/exercises/Exercise04.java\nand src/main/java/mt/ImageTransformer.java\n\n0. Getting started\n1 Point\n\n\nFor Exercise 4 we provide a GUI that displays the image with different image transformation options.\n\n\n\nOnce you have all the transformations implemented you should be able to adjust the sliders and perform the desired transformations in an interactive manner.\n\n\nThe transformations requires an origin point about which we perform all the transformation.\n\n\nExtend the Image class with these three methods\n\n\n\n    // store the origin points x,y as \n    // a class variable\n    public void setOrigin(float x, float y)\n\n    // the origin() returns the {x,y} as float \n    // array from the stored origin class variable. \n    public float[] origin()\n\n    // Sets the origin to the center of the image\n    public void centerOrigin()\n\n\nTo ensure that everything is running, run the main function.\nWe already set the origin point for you in the file src/main/java/exercises/Exercise04.java\nTo ensure that everything is running, run the main function of Exercise04.\n\n1. Image interpolation\n4 Points\n\n\nSince the image transformations heavily relies on the interpolation, we first implement the interpolation method by extending the Image class  with the following method:\n\n\npublic float interpolatedAt(float x, float y)  \n\n\nThe method takes in a physical $(x,y)$ coordinate and returns the image intensity at that position.\nWe use bilinear interpolation to find the value at $(x,y)$ (described in the theory).\n\n\nWe can rewrite the interpolation equation using the linear interpolation formula when we want to interpolate between two points $x_1,x_2$ with function value $f(x_1),f(x_2)$ to find out the function value $f(x)$ at $x$.\n\n\n$$ \\frac{f(x) - f(x_1)}{x-x_1} = \\frac{f(x_2) - f(x_1)}{x_2 - x_1} $$\n\n\n\n\nSince we already know the difference $x_2 - x_1$ is either 1.0 if we have a pixel spacing of 1.0 or pixel spacing, we can simplify the above equation as follows:\n\n$$f(x) = f(x_1) + (x-x_1) (f(x_2) - f(x_1))$$\n\n\n\nYou can use the function below to compute linear interpolation between two points $x_1,x_2$ at $x$\n\n\n // Definition of arguments\n // diff_x_x1 = x - x_1 compute the difference between point x and x_1\n // fx_1 = f(x_1), pixel value at point x_1\n // fx_2 = f(x_2), pixel value at point x_2 \n\n float linearInterpolation(float fx_1, float fx_2, float diff_x_x1) {\n     return fx_1 + diff_x_x1 * (fx_1 - fx_2);\n }\n \n\n\nWe now have an way to interpolate between two points in 1D. We need to extend this to 2D case such that we can use \nit for interpolating values in our image. An illustration of how this can be done is \nalready given in the theory section.\n\n\nImplementation detail We describe here possible way to implement the interpolation scheme.\n\n\nFind the 4 nearest pixel indices, for the given physical coordinate $(x,y)$. To do, this you have to transform\nthe physical coordinate to the index space of the image.\n\n\nHint: In physical space all the values of $x$ and $y$\nare computed from origin. So we just need to subtract the origin from the coordinates for this correction.\n\nx -= origin[0]\ny -= origin[1]\n\n\nPixel spacing also alters the physical coordinates and needs to be corrected for. \nThis can be done using just by dividing each coordinate by the pixel spacing.\n\nx /= spacing;\ny /= spacing\n\n\nHint: Since each pixel is a unit square you can round up and down each coordinate ($x$ and $y$) separately \nto get the 4 nearest pixels coordinates.\n\n\nInterpolate along an axis (here we choose the x-axis) initially using the linear interpolation \nfunction to obtain intermediate points.\n\n\nNow interpolate along the intermediate points (i.e you are interpolating along y-axis)\n\n\nNote: Take care of image origin and pixel spacing for the input coordinates before you perform any of the steps.\nAlso, always use atIndex and setIndex for accessing the image values. \nThis ensures that we handle the values at boundary correctly.\n\n\n\n\nExample:\nHere we look at a single point to understand how to implement our algorithm\n\n\nIf we have an input $(x,y) = (0.4,0.4)$, then the 4 nearest pixel coordinates are $(0,0)$,$(1,0),(1,1),(0,1)$\n\n\nInterpolating the values between the points $a = (0,0)$, $b = (1,0)$, find the intermediate \nvalue at point $I_1 = (0.4,0)$.\n\n\nSimilarly interpolate between $c = (0,1)$ and $d = (1,1)$ to find the intermediate value at point $I_2 = (0.4,1)$.\n\n\nNow we can just use the values at the intermediate points $I_1 = (0.4,0)$ and $I_2 = (0.4,1)$ and \nperform a linear interpolation in the y direction to obtain the final result at $(0.4,0.4)$.\n\n\n\n\n2. Image Transformation\n5 Points\nNow we can start with the implementation of ImageTransformer class.\n\nThe class consists of the following member functions for translation\n\n\n// Transformation parameters\npublic float shiftX; // tx\npublic float shiftY; // ty\npublic float rotation; // theta\npublic float scale; // s\n\n\n\nAlso use the interface ImageFilter abstract class which you have implemented in the previous exercises. \nThis can be done using implements keyword.\n\n\nAdd the method apply(Image input,Image output) which takes in two variables input and \noutput of Image class type. The input variable provides the input image to our transformer class. \nThe output variable is where the transformed image is stored.\n\n\nConsider each pixel in the image with index $(i,j)$. When we access an image pixel we get \nthe pixel intensity stored at the location $(i,j)$.\n\n\nHere $(i,j)$ represents the image coordinates $(x,y)$ and the pixel value at $(i,j)$ represents $f(x,y)$.\n\n\nWe want to transform $(x,y) \\to (x',y')$ and find the pixel value at the new location for a \ngiven set of input transformation parameters $t_x,t_y,\\theta,s$ to transform the input image coordinate $(x,y)$.\n\n\nLet us go over a possible approach to implement the apply method which \nimplements (translation,rotation and scaling). In addition, once we have the transformed coordinates $(x',y')$ we \ninterpolate the value at this coordinate to set the output value of the new image.\n\n\nWe can implement the transformations and interpolation using the equations defined \nin the theory section. \n\n\nHowever, from the implementation perspective it is much easier to ask what will be my output image value \nat the current position $(x',y')$  for the given  transformations parameters.\n\n\nFor this we need to find the input coordinate $(x,y)$ for the given transformation parameters.\nThis mapping from $(x',y') \\to (x,y)$ is known as the inverse transformation.\n\n\nJust to recap our current aim is to iterate over the output image along each \npixel $(i,j)$ (also referred as $(x',y')$) and find the inverse transformation (x,y).\nOnce we find $(x,y)$ we can just interpolate the values in the input image at $(x,y)$ and\nset it to the output image value at (x',y').\n\n\nAn example code to accomplish this looks like below:\n\n\n\n// We need to compute (x,y) from (x&#39;,y&#39;)\n// We use xPrime,yPrime in the code to indicate (x&#39;,y&#39;)\n// Interpolate the values at (x,y) from the input image to get\nfloat pixelValue = input.interpolatedAt(x,y);\n\n// Set your result at the current output pixel (x&#39;,y&#39;)\noutput.setAtIndex(xPrime, yPrime, pixelValue);\n\n\n\n\nThe inverse transformations can be computed using the following equations.\n\n\nTranslation\n\n$x =  x' - t_x$\n$y =  y' - t_y$ \n\n\n\nRotation\n\n$x= x' \\cos\\theta + y' \\sin\\theta$\n$y= - x \\sin\\theta + y' \\cos\\theta$\n\n\n\nScaling\n\n$x=  \\frac{x'}{s}$\n$y=  \\frac{y'}{s}$\n\n\n\nImplementation detail Now you can directly use the above equations to implement translation, rotation and scaling.\nThe entire apply method for the ImageTransformer class can be implemented as follows:\n\n\nIterate over each pixel in the output image (although they are just the same as input initially).\n\n\nAt each pixel the index $(i,j)$ represents our coordinates $(x',y')$ of the output image\n\n\nApply the transformations using the equations described above to find $(x,y)$\n\n\nNow set the output image value at $(i,j)$ (also referred as (x',y')) from the interpolated values at $(x,y)$ \nfrom the input image.\n\n\nUse the setIndex() for setting the values of the output image and atIndex() for getting the values \nfrom input image.\n\n\nIn the above formulation we assume that we have pixel spacing of $spacing = 1.0$ and the \nimage origin at $(x_0, y_0) = (0,0)$.\n\n\nYou can extend this to work for different values of pixel spacing and origin.\n\n\nHint: Think of pixel spacing as a scaling and origin as a translation transformation. \n(apply both spacing and origin transformation to the input coordinates $(x,y)$ as  $(x * px , y * py) + (x_0,y_0))$ \n\n\n\n\n"
},

{
"title": "Exercise 3",
"url": "https://mt2-erlangen.github.io/archive/2020/exercise-3/",
"body": "Submission deadline: 25.05.20 23:59h\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nImages and 2-d Convolution\nIn this exercise, we finally to work with images. It's time to update the file src/main/java/lme/DisplayUtils.java to the newest version.\nThis should provide you the following methods to work with images:\n\n    // Open a file\n    public static mt.Image openImage(String path) \n\n    // Download and open a file from the internet\n    public static mt.Image openImageFromInternet(String url, String filetype) \n\n    // Save an image to a file\n    public static void saveImage(mt.Image image, String path) \n\n    //  Show images\n    public static void showImage(float[] buffer, String title, int width) \n    public static void showImage(float[] buffer, String title, long width, float[] origin, double spacing, boolean replaceWindowWithSameName)\n\n\nThey all work with the class mt.Image so let's create it!\nBefore that, add the following two methods to your Signal class (they are used by the tests of this exercise):\n\n    // Needs: import java.util.Random\n    public void addNoise(float mean, float standardDeviation) {\n\tRandom rand = new Random();\n\tfor (int i = 0; i &lt; buffer.length; i++) {\n\t    buffer[i] += mean + rand.nextGaussian() * standardDeviation;\n\t}\n    }\n\n    public void setBuffer(float[] buffer) {\n\tthis.buffer = buffer;\n    }\n\nPS: The method addNoise is also useful to test your mean and standardDeviation calculation in exercise 2.\nCreate a long signal and add noise with a specific mean and standardDeviation.\nThe result of your mean and standardDeviation method should be approximatelly the same.\nmt/Image.java\n4 Points\nThe code for this section should go to src/main/java/mt/Image.java\nOur goal is to share as much code with our mt.Signal class. So mt.Image will be a subclass of mt.Signal.\n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\nimport lme.DisplayUtils;\n\npublic class Image extends Signal {\n\n\n}\n\nmt.Image has five members (apart from the ones inherited by mt.Signal).\n\n    // Dimensions of the image\n    protected int width; \n    protected int height; \n\n    // Same as Signal.minIndex but for X and Y dimension\n    protected int minIndexX;\n    protected int minIndexY;\n\n    // For exercise 4 (no need to do anything with it in exercise 3)\n    protected float[] origin = new float[]{ 0, 0 };\n\nAnd two constructors:\n\n    // Create an image with given dimensions\n    public Image(int width, int height, String name)\n\n    // Create an image with given dimensions and also provide the content\n    public Image(int width, int height, String name, float[] pixels)\n\nAs shown in the exercise slides, we will store all the pixels in one array, like we did in Signal.\nThe array should have the size width * height.\nminIndexX,minIndexY should be 0 for normal images.\n\n\nCall the constructors of the super class Signal in the constructors of Image.\nYou can call the constructor of a super class by placing super(...) with the respetive arguments in the first line of the constructor of the subclass.\nThe constructor public Image(int width, int height, String name, float[] pixels) does not need to create its own array (take pixels for buffer).\nBut you can check whether pixels has the correct size.\nLet's also provide some getters!\n\n    // Image dimensions\n    public int width()\n    public int height()\n\n    // Minimum and maximum indices (should work like Signal.minIndex/maxIndex)\n    public int minIndexX()\n    public int minIndexY()\n    public int maxIndexX()\n    public int maxIndexY()\n\natIndex and setAtIndex should work like in Signal except that they now have two coordinate indices.\natIndex should return 0.0f if either the x or y index are outside of the image ranges.\n\n    public float atIndex(int x, int y)\n    public void setAtIndex(int x, int y, float value) {\n\nRemember how we calculated the indices in the exercise slides. You have to apply that formula in atIndex/setAtIndex.\n\n\nAdd the method show to display the image\n\n    public void show() {\n        DisplayUtils.showImage(buffer, name, width(), origin, spacing(), /*Replace window with same name*/true);\n    }\n\nOpen the image pacemaker.png in a file  src/main/java/exercise/Exercise03 (in the same project as previous exercise):\n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage exercises;\n\nimport mt.GaussFilter2d;\nimport mt.Image;\n\npublic class Exercise03 {\n    public static void main(String[] args) {\n        (new ij.ImageJ()).exitWhenQuitting(true);\n\n        Image image = lme.DisplayUtils.openImageFromInternet(&quot;https://mt2-erlangen.github.io/pacemaker.png&quot;, &quot;.png&quot;);\n        image.show();\n\n    }\n}\n\nThe image is from our open access book.\n\nmt.ImageFilter\n3 Points:\nLike in Exercise 1, we want to be able to convolve our image signal.\nInfact, we will learn a lot of new ways to process images.\nOften, we need to create an output image of same size.\nLet's create an interface (src/main/java/mt/ImageFilter.java) for that, so we only need to implement this once.\n\npackage mt;\n\npublic interface ImageFilter {\n    default mt.Image apply(mt.Image image) {\n        Image output = new Image(image.width(), image.height(), image.name() + &quot; processed with &quot; + this.name());\n        apply(image, output);\n        return output;\n    }\n\n    default void apply(mt.Image input, mt.Image output) {\n        throw new RuntimeException(&quot;Please implement this method!&quot;);\n    }\n\n    String name();\n}\n\nThe code for the convolution should go to src/main/java/mt/LinearImageFilter.java\nOk. Now the convolution. The class has already a method that we will need later. It uses your sum method.\n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\npublic class LinearImageFilter extends Image implements ImageFilter {\n\n    public void normalize() {\n\tdouble sum = sum();\n\tfor (int i = 0; i &lt; buffer.length; i++) {\n\t    buffer[i] /= sum;\n\t}\n    }\n}\n\nCreate a constructor for it. Recall how we implemented LinearFilter!\nminIndexX and minIndexY need to be set to $-\\lfloor L_x/2 \\rfloor$ and $-\\lfloor L_y/2 \\rfloor$ when $L_x$ is the\nfilter's width and $L_y$ the filter's height.\n\n    public LinearImageFilter(int width, int height, String name)\n\nConvolution in 2-d works similar to convolution in 1-d.\n$$K_x = \\lfloor L_x/2 \\rfloor$$\n$$K_y = \\lfloor L_y/2 \\rfloor$$\n$$g[x,y] = \\sum_{y'=-K_y}^{+K_y} \\sum_{x'=-K_x}^{+K_x} f[x-x', y-y'] \\cdot h[ x', y' ] $$\n$$g[x,y] = \\sum_{y'=\\text{h.minIndexY}}^{\\text{h.maxIndexY}} \\sum_{x'=\\text{h.minIndexX}}^{\\text{h.maxIndexX}} f[x-x', y-y'] \\cdot h[ x', y' ] $$\nRemember to use atIndex and setAtIndex to get and set the values.\nImplement the convolution in the method apply.\nThe result image was already created by our interface ImageFilter.\n\n    public void apply(Image image, Image result)\n\n\n\nSource: https://github.com/vdumoulin/conv_arithmetic\nNow it's time to test!\nUse the file src/test/java/mt/LinearImageFilterTests.java.\nGauss Filter\n2 Points\nThe code for the Gauss filter should go to src/main/java/mt/GaussFilter2d.java.\nThe Gauss filter is a LinearImageFilter with special coefficients (the filter has the same height and width).\n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\npublic class GaussFilter2d extends LinearImageFilter {\n    \n}\n\nIt has the following constructor\n\n    public GaussFilter2d(int filterSize, float sigma)\n\nIn the constructor, set the coefficients according to the unormalized 2-d normal distribution with standard deviation $\\sigma$ (sigma).\nMath.exp is the exponetial function.  Use setAtIndex: $x$ should run from minIndexX to maxIndexX and $y$ from minIndexY to maxIndexY.\n$$ h[x,y] = \\mathrm{e}^{-\\frac{x^2+y^2}{2 \\sigma^2}}$$\nCall normalize() at the end of the constructor to ensure that all coefficients sum up to one.\nTest your Gauss filter in Exercise03.java.\nUse arbitray values for sigma and filterSize.\nThe Gauss filter will blur your input image clearly if you chose a large value for sigma.\n\nThere is also a unit test file that you can use: src/test/java/mt/GaussFilter2dTests.java\nCalculating with Images\n1 Points\nThe code for this section should go to src/main/java/mt/Image.java.\nImplement the method Image.minus in Image.java that subtracts the current image element-wise with another one and returns the result:\n\n    public Image minus(Image other)\n\nWe use this method to calculate error images.\nYou can implement this with only one loop over the elements of the buffers of the two images.\nDemo\nThis is not required for the exercise!\nPlace the file src/main/java/exercises/Exercise03Demo.java\nin your project folder and run it.\n\nYou should see an interactive demo applying your Gauss filter to a noisy image.\nYou change change the used parameters.\nSubmitting\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nThen, compress your source code folder src to a zip archive (src.zip) and submit it on studOn.\n\n\n\n\n\n"
},

{
"title": "Exercise 2",
"url": "https://mt2-erlangen.github.io/archive/2020/exercise-2/",
"body": "\n\n\n\n\nSubmission deadline: 18.05.20 23:59h\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nStatistical Measures\nIn this exercise, we want to have a look on how we can analyze signals using simple statistical measures.\nWe will use a freely available ECG data set with the goal to distinguish healthy from patients with heart rythm problems.\n\nYou can find the original data set here\nbut we recommend to use a post-processed version available on studOn.\nGradle Build System\nIn Medizintechnik II we use the build system Gradle.\nGradle is especially popular for Android projects since it's easy to add new software dependencies that will be automatically\ndownloaded.\nIn our case, the published data set is saved as Matlab *.mat files.\nTo read those files, an external dependency was already added to our build.gradle file.\n\n    implementation &#39;us.hebi.matlab.mat:mfl-core:0.5.6&#39;\n\ndoes the magic and automatically downloaded a *.mat file reader.\nIn case, you need to add external software to your own projects you can use this search engine.\nTasks\nLoading one of File of the Data Set\nLoad the file src/main/java/exercises/Exercise02.java (available here (Click the raw button)) into your existing project.\nIt alread contain some code for parsing the program parameters:\n\n    public static void main(String[] args) throws IOException {\n\t(new ij.ImageJ()).exitWhenQuitting(true);\n\n\tSystem.out.println(&quot;Started with the following arguments:&quot;);\n\tfor (String arg : args) {\n\t    System.out.println(arg);\n\t}\n\n\tif (args.length == 1) {\n\t    File file = new File(args[0]);\n\t    if (file.isFile()) {\n\t\t// Your code here:\n\n\n\t    } else {\n\t\t    System.err.println(&quot;Could not find &quot; + file);\n\t    }\n\n\t} else {\n\t    System.out.println(&quot;Wrong argcount: &quot; + args.length);\n\t    System.exit(-1);\n\t}\n\nLaunch Exercise02 with the one of the files of the data set as an argument (e.g. &lt;where_you_saved_your_data_set&gt;/MLII/1 NSR/100m (0).mat)!\n\nHow to do that in Eclipse\nHow to do that in IntelliJ\n\nYour program should print now the file name you selected:\n\nRemember to never put file names directly in your code. Your program will then only work on your machine!\nLet's open this file!\n\nif (file.isFile()) {\n    // A file should be opened \n    us.hebi.matlab.mat.types.Matrix mat = Mat5.readFromFile(file).getMatrix(0);\n    Signal heartSignal = new mt.Signal(mat.getNumElements(), &quot;Heart Signal&quot;);\n    for (int i = 0; i &lt; heartSignal.size(); ++i) {\n\t    heartSignal.buffer()[i] = mat.getFloat(i);\n    }\n    heartSignal.show();\n\n\n} else if (file.isDirectory()) {\n\nYou should now see the signal. However, this plot does not have any labels with physical units attached.\nWe will change that later.\n\nExtension of Signal.java\n4 Points\nTo analyze this and other signals, we will extend our Signal class.\nPlease implement the following methods in Signal.java that calculate some descriptive properties of the signal:\n\n    public float min()        //&lt; lowest signal value\n    public float max()        //&lt; largest signal value\n    public float sum()        //&lt; sum of all signal values\n    public float mean()       //&lt; mean value of the signal\n    public float variance()   //&lt; variance of the signal\n    public float stddev()     //&lt; standard deviation of the signal\n\nTest the methods in your main function and check whether the calculated values seem plausible\nby looking at your plot and printing the calculated values.\nPhysical Dimensions\n1 Points\nThe code for this section belong to Signal.java\nIn the last exercise, we treated signals as pure sequence of numbers without any physical dimensions.\nBut for medical measurements physical dimensions are important.\nWe want to extend our plot to look like this with the horizontal axis labeled with seconds:\n\nTo do this we will add a new member to our signal that's describing the physical distance between two samples\n\n    protected float spacing = 1.0f;  //&lt; Use 1.0f as a default when we don&#39;t set the physical distance between points\n\nAdd also a setter and getter method\n\n    public void setSpacing(float spacing) \n    public float spacing() \n\nRead in the discription of the data set the sampling frequency of the signal\nand use it to calculate the spacing between two samples. Set this property setSpacing in the main method.\nNext, we want to change show() to regard our spacing and to accept a ij.gui.Plot so that we can set the axis of our plot.\n\n    public void show(Plot plot) {\n\t    DisplayUtils.showArray(buffer, plot, /*start of the signal=*/0.f, spacing);\n    }\n\nBecause we are lazy, we can still keep the original usage of show()\n\n    public void show() {\n\t    DisplayUtils.showArray(buffer, name, , /*start of the signal=*/0.f, spacing);\n    }\n\nPlease create an instance of ij.gui.Plot in the main method of Exercise02 with descriptive labels for both axis and use if for heartSignal.show(...).\n\n\n// Constructs a new Plot with the default options.\nPlot plot = new Plot(&quot;chosee title here&quot;, &quot;choose xLabel here&quot;, &quot;choose yLabel here&quot;)\nheartSignal.show(plot);\n\n//... add here more plotting stuff\n\nplot.show()\nDetermine the Heart Frequency\n5 Points\nThe remainder of this exercise will be implemented in Exercise02.java\nCreate a file  src/main/java/lme/HeartSignalPeaks.java with following content\n\npackage lme;\n\nimport java.util.ArrayList;\n\npublic class HeartSignalPeaks {\n\tpublic ArrayList&lt;Double&gt; xValues = new ArrayList&lt;Double&gt;();\n\tpublic ArrayList&lt;Double&gt; yValues = new ArrayList&lt;Double&gt;();\n}\n\nArrayList behave like arrays, except you can add new items to make it longer. You can read more about them here.\nWe now want to find the peaks of the heart signal. We do that by finding local maxima within region that are above a certain\nthreshold (here in blue).\nFind a good value of this threshold so that all peaks are above this value.\nYou may use mean(), max(), min() to calculate it.\nYou can see your threshold by ploting it:\n\n    plot.setColor(&quot;blue&quot;);\n    plot.add(&quot;lines&quot;, new double[] { 0, /* a high value */10000 }, new double[] { threshold, threshold });\n\n\nImplement the following method that finds all peaks of the signal.\n\n    public static lme.HeartSignalPeaks getPeakPositions(mt.Signal signal, float threshold)\n\nTo determine the signal peaks, one can use normal maximum search over the signal values.\nSave the found maximum value (i.e. signal amplitude) in x(max) and\nthe location of maximum (i.e. the time at which the peak occurs) in y(arg max).\nYou can implement the peak finding method as follows:\n\n\nLoop over the signal and at each index\n\n\nUse  boolean variable to determine if the current signal value is above the threshold.\n\n\nIf the previous signal value was above the threshold (i.e boolean value was true), and the current value is below threshold (i.e boolean value is false)\n\n\nAdd the previous signal value as a instance of HeartSignalPeaks (like peaks.xValues and peaks.yValues)\n\n\nThis is a suggested workflow, but feel free to use your own ideas to efficently find the peaks of the signal.\nYou can plot the peaks you have found:\n\n    plot.setColor(&quot;red&quot;);\n    plot.addPoints(peaks.xValues, peaks.yValues, 0);\n\nNext, create a Signal with the difference in time between succesive peaks (import import java.util.ArrayList;). \n\n\tpublic static mt.Signal calcPeakIntervals(lme.HeartSignalPeaks peaks) {\n\t\tArrayList&lt;Double&gt; peakPositions = peaks.xValues;\n\t\tif  (peakPositions.size() &gt; 1) {\n\t\t\tSignal intervals = new mt.Signal(peaks.xValues.size() - 1, &quot;Peak Intervals&quot;);\n\n\t\t\tfor (int i = 0; i &lt; peakPositions.size() - 1; ++i) {\n\t\t\t\tintervals.buffer()[i] = (float) (peakPositions.get(i + 1) - peakPositions.get(i));\n\t\t\t}\n\t\t\treturn intervals;\n\t\t} else {\n\t\t\treturn new mt.Signal(1, &quot;No Intervals found&quot;);\n\t\t}\n\t}\n\nYou can use that signal to determine the mean cycle duration (peakIntervals.mean()), the mean heart frequency ((1. / intervals.mean())) and\nbeats per minute (60. * 1. / intervals.mean()). Print those values!\nSummary of tasks\nTo summarize the list of tasks that needs to be implemented to complete this exercise\n\nSet the file path correctly to load the signal into your program (Ensures you can load the signal inside the program)\nAdd labels to the plot, include spacing variable in signal class for visualizing plots in physical dimensions.\nImplement methods to compute statistical measures (like mean, median,...). (Use the formula provied in lecture/exercise slides)\nDetermine the threshold (follow the description provided here)\nFind the peaks (follow the description provided here)\nCalculate intervals between the peaks\n\nNote\nWhile setting file path as arguments, add &quot;path&quot; if there are spaces in file name since java parses space as new arguments.\nBonus\nThis is not required for the exercise.\nRun Exercise02 with other files of the data set as an argument.\nWhat is the meaning of the mean value and the variance of time distance between the peeks?\nHow do signals with low variance in the peak distances look like and how signals with high variance?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
},

{
"title": "Exercise 1",
"url": "https://mt2-erlangen.github.io/archive/2020/exercise-1/",
"body": "Signals and Convolution\nSubmission deadline: 11.05.20 23:59h\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nEach exercise has 10 points. You have to achieve 30 of 60 points in six homework exercises to pass the module.\nImageJ\nThe image processing program we want to use during this semester is called ImageJ.\nIt was developed at the US National Institutes of Health and is used nowadays especially in research \nfor medical and biological images.\nIf you want to, you can download a stand-alone version of the program here.\nGetting started\nImageJ can also be used as a Java library.\nWe already created a Java project that uses ImageJ.\nYou can download it from https://github.com/mt2-erlangen/exercises-ss2021 and import with the IDE of your choice:\n\nInstructions for Eclipse\nInstructions for IntelliJ\n\nTasks\n\n\nYou should now be able to execute the file src/main/java/exercises/Exercise01.java\n\n\nThe following code is opening the ImageJ main window and exits the running program when the window is closed.\n\npublic class Exercise01 {\n    public static void main(String[] args) {\n        (new ij.ImageJ()).exitWhenQuitting(true);\n\n    }\n}\n\nIntelliJ will only allow to run Exercise01 when there are no errors in the project. You can just out-comment the method lme.Algorithms.convolution1d until you implemented your Signal class.\nSignal.java\n4 Points\nAs a first step, we will implement the class Signal \nwhich should hold a signal of finite length.\nCreate the file src/main/java/mt/Signal.java.\n\n// &lt;your name&gt; &lt;your idm&gt;\n// &lt;your partner&#39;s name&gt; &lt;your partner&#39;s idm&gt; (if you submit with a group partner)\npackage mt;\n\nimport lme.DisplayUtils;\nimport ij.gui.Plot;\n\npublic class Signal {\n\n}\n\nSignal should have the following members\n\n    protected float[] buffer; // Array to store signal values\n    protected String name;    // Name of the signal\n    protected int minIndex;   // Index of first array element (should be 0 for signals)\n\nImplement two constructors for Signal\n\n    public Signal(int length, String name)     // Create signal with a certain length (set values later)\n    public Signal(float[] buffer, String name) // Create a signal from a provided array\n\nImplement the following getter methods for Signal\n\n    public int size()        // Size of the signal\n    public float[] buffer()  // Get the internal array \n    public int minIndex()    // Get lowest index of signal (that is stored in buffer)\n    public int maxIndex()    // Get highest index of signal (that is stored in buffer)\n    public String name()     // Get the name of the signal\n\nNext, we want to visualize our Signal in the method show. You can use provided function lme.DisplayUtils.showArray.\nTo test it, create a Signal with arbitray values in the main method of Exercise01 and call its show method.\n\n    public void show() {\n        DisplayUtils.showArray(this.buffer, this.name, /*start index=*/0, /*distance between values=*/1);\n    }\n\nIn our black board exercises, we agreed that we want to continue our signals with zeros where we don't have any values stored.\nIf we access indices of our Signal with values smaller than minIndex() or larger maxIndex() we want to return 0.0f.\nIf a user accesses an index between minIndex() and maxIndex() we want to return the corresponding value stored in our array.\n\nImplement the method atIndex and setAtIndex. Please be aware that minIndex can be smaller than 0 for subclasses of Signal.\nIf setAtIndex is called with an invalid index (smaller than minIndex or greater than maxIndex), it's ok for the program to crash.\nThis should not happen for atIndex.\n\n    public float atIndex(int i)\n    public void setAtIndex(int i, float value)\n\nYou can check the correctness of atIndex/setAtIndex with the test testAtIndex in file src/test/java/SignalTests.java.\nLinearFilter.java\n3 Points\nImplement LinearFilter in file src/main/java/LinearFilter.java as a subclass of Signal.\nLinearFilter should work like Signal except its minIndex should be at - floor(coefficients.length/2) as in the exercise slides.\n\nLinearFilter should have a constructor that checks that coefficients is an array of odd size or throws an error otherwise (any error is ok).\n\n    public LinearFilter(float[] coefficients, String name)\n\nand a method that executes the discrete convolution on another Signal input and returns an output of same size.\n\n   public Signal apply(Signal input);\n\nYou should be able to directly use the formula from the exercise slides (f is the input signal, h our filter, $L$ the filter length)\n$$K = \\lfloor L/2 \\rfloor$$\n$$g[k] = \\sum_{\\kappa=-K}^{K} f[k-\\kappa] \\cdot h[ \\kappa ]$$\nor with our minIndex/maxIndex methods for each index $k$ of the output signal.\n$$g[k] = \\sum_{\\kappa=h.\\text{minIndex}}^{h.\\text{maxIndex}} f[k-\\kappa] \\cdot h[\\kappa] $$\nBe sure that you use atIndex to access the values of input and the filter.\n\nYou can test your convolution function with the tests provided in src/test/java/LinearFilterTests.java.\nGood test cases are:\n\n{0,0,1,0,0}: this filter should not change your signal at all\n{0,1,0,0,0}: this filter should move your signal one value to the left\n{0,0,0,1,0}: this filter should move your signal one value to the right\n\nQuestions\n3 Points\nIn this task we want to convolve a test Signal with three different linear filters.\nFilter the signal $f[k]$  Signal(new float[]{0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0}, &quot;f(k)&quot;)\nwith filters\n\n$h_1[k]$: {1.0f/3 ,1/3.f ,1/3.f},\n$h_2[k]$: {1/5.f, 1/5.f , 1/5.f, 1/5.f, 1/5.f},\n$h_3[k]$: {0.5f, 0, -0.5f}.\n\nSave the images of the input signal and filtered results (recommended filetype: png).\nCreate a PDF document (e.g. with Word or LibreOffice) with those images in which you describe briefly how the filters modified the input signal and why.\nSubmitting\nPlease ensure that all files you created also contain your name and your IDM ID and also your partner's name and IDM ID if you're not working alone.\nThen, compress your source code folder src to a zip archive (src.zip) and submit it and your PDF document via StudOn!\n"
}
]
